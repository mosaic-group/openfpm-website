{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenFPM is a scalable open framework for particle and particle-mesh codes on parallel computers. Important Announcements : OpenFPM annual hackathon in Spring 2024! Features Scalable serial and parallel data structures for heterogeneous computing systems available CPU and GPU-accelerated hardware Operators for particle methods linear differential discretization, e.g. DC-PSE, SPH Particle-mesh and mesh-particle interpolation schemes Data structures for efficient particle methods simulations, e.g. Cell-List, Verlet-List , for efficient item-based, e.g. Lennard-Jones molecular dynamics , and continuous simulations Sparse grid on CPU and GPU Support for PETSc , Eigen linear algebra backends Support for ODE integration operators with Boost.Numeric.Odeint Level-set formulation with Algoim GPU execution backends include CUDA , HIP , OpenMP , alpaka ... and many others Contact Use the GitHub issue tracker to report bugs/questions/comments. See the Cite us page for the citation information. Documentation Building from source \u250a Using Docker \u250a Examples \u250a Videos \u250a Doxygen Selected simulations OpenFPM is used in many projects, please see some of the simulation samples below. Other examples are available in the Example section . Diffusive heat conduction with distributed surface sink in a sample of reticulate porous ceramics. Reconstruction of the solid phase represented as a sparse-grid level set Visualization of the OpenFPM-simulation of one of steady-state patterns produced by the Gray-Scott reaction-system in 3D. Dam-break simulation of weakly compressible Navier-Stokes equations in SPH formulation. The \ufb01gure shows a density iso-surface indicating the \ufb02uid surface with color indicating the \ufb02uid velocity magnitude. The simulation domain is split into 4 subdomains run on 4 processors. Fully developed bent coherent angular motion of an active fluid in a 3D annular domain with visualized polarity (top) and velocity (bottom) streamlines. OpenFPM simulation of a vortex ring at with 256 million particles on 3072 processors using a hybrid particle-mesh Vortex Method to solve the incompressible Navier-Stokes equations","title":"Home"},{"location":"#features","text":"Scalable serial and parallel data structures for heterogeneous computing systems available CPU and GPU-accelerated hardware Operators for particle methods linear differential discretization, e.g. DC-PSE, SPH Particle-mesh and mesh-particle interpolation schemes Data structures for efficient particle methods simulations, e.g. Cell-List, Verlet-List , for efficient item-based, e.g. Lennard-Jones molecular dynamics , and continuous simulations Sparse grid on CPU and GPU Support for PETSc , Eigen linear algebra backends Support for ODE integration operators with Boost.Numeric.Odeint Level-set formulation with Algoim GPU execution backends include CUDA , HIP , OpenMP , alpaka ... and many others","title":"Features"},{"location":"#contact","text":"Use the GitHub issue tracker to report bugs/questions/comments. See the Cite us page for the citation information.","title":"Contact"},{"location":"#documentation","text":"Building from source \u250a Using Docker \u250a Examples \u250a Videos \u250a Doxygen","title":"Documentation"},{"location":"#selected-simulations","text":"OpenFPM is used in many projects, please see some of the simulation samples below. Other examples are available in the Example section . Diffusive heat conduction with distributed surface sink in a sample of reticulate porous ceramics. Reconstruction of the solid phase represented as a sparse-grid level set Visualization of the OpenFPM-simulation of one of steady-state patterns produced by the Gray-Scott reaction-system in 3D. Dam-break simulation of weakly compressible Navier-Stokes equations in SPH formulation. The \ufb01gure shows a density iso-surface indicating the \ufb02uid surface with color indicating the \ufb02uid velocity magnitude. The simulation domain is split into 4 subdomains run on 4 processors. Fully developed bent coherent angular motion of an active fluid in a 3D annular domain with visualized polarity (top) and velocity (bottom) streamlines. OpenFPM simulation of a vortex ring at with 256 million particles on 3072 processors using a hybrid particle-mesh Vortex Method to solve the incompressible Navier-Stokes equations","title":"Selected simulations"},{"location":"about/","text":"About OpenFPM OpenFPM is an open-source C++ framework for parallel particles-only and hybrid particle-mesh codes. OpenFPM is intended as a successor to the discontinued PPM Library 1 , 2 . Please cite with: @article{INCARDONA2019155, title = {OpenFPM: A scalable open framework for particle and particle-mesh codes on parallel computers}, journal = {Computer Physics Communications}, volume = {241}, pages = {155-177}, year = {2019}, issn = {0010-4655}, doi = {https://doi.org/10.1016/j.cpc.2019.03.007}, url = {https://www.sciencedirect.com/science/article/pii/S0010465519300852}, author = {Pietro Incardona and Antonio Leo and Yaroslav Zaluzhnyi and Rajesh Ramaswamy and Ivo F. Sbalzarini}, Other publications utilizing OpenFPM Wimmer A, Panzer H, Zoeller C, Adami A, Adams N A & Zaeh M F, Experimental and numerical investigations of the hot cracking susceptibility during the powder bed fusion of AA 7075 using a laser beam , Progress in Additive Manufacturing, 2023 A. Singh, P. H. Suhrcke, P. Incardona, I. F. Sbalzarini, A numerical solver for active hydrodynamics in three dimensions and its application to active turbulence , Physics of Fluids, 2023 Singh, A., Foggia, A., Incardona, P. et al, A Meshfree Collocation Scheme for Surface Differential Operators on Point Clouds , Journal of Scientific Computing, 2023 Incardona P, Gupta A, Yaskovets S, Sbalzarini IF, A portable C++ library for memory and compute abstraction on multi-core CPUs and GPUs , Concurrency and Computation Practice and Experience 2023 C. Z\u00f6ller, N.A. Adams, S. Adami, Numerical investigation of balling defects in laser-based powder bed fusion of metals with Inconel 718 , Additive Manufacturing, 2023 Geara, S., Martin, S., Adami, S. et al. SPH 3D simulation of jet break-up driven by external vibrations , Cumputational Particle Mechanics, 2023 Singh, Abhinav and Vagne, Quentin and Julicher, Frank and Sbalzarini, Ivo F, Spontaneous flow instabilities of active polar fluids in three dimensions , Physical Review Research, 2023 Veettil, Sachin Krishnan Thekke and Zavalani, Gentian and Acosta, Uwe Hernandez and Sbalzarini, Ivo F. and Hecht, Michael, Global Polynomial Level Sets for Numerical Differential Geometry of Smooth Closed Surfaces , SIAM Journal on Scientific Computing, 2023 C. Z\u00f6ller, N.A. Adams, S. Adami, A partitioned continuous surface stress model for multiphase smoothed particle hydrodynamics , Journal of Computational Physics, 2023 Maddu Suryanarayana, Cheeseman Bevan L., Sbalzarini Ivo F. and M\u00fcller Christian L., Stability selection enables robust learning of differential equations from limited noisy data , Proceedings Of The Royal Society A Stoyanovskaya, O.P., Grigoryev, V.V., Suslenkova, A.N., Davydov, M.N., Snytnikov, N.V., Two-Phase Gas and Dust Free Expansion: Three-Dimensional Benchmark Problem for CFD Codes , Fluids 2022 Nesrine Khouzami, Friedrich Michel, Pietro Incardona, Jeronimo Castrillon, Ivo F. Sbalzarini, Model-based autotuning of discretization methods in numerical simulations of partial differential equations , Journal of Computational Science, 2022 Wimmer A, Yalvac B, Zoeller C, Hofstaetter F, Adami S, Adams NA, Zaeh MF., Experimental and Numerical Investigations of In Situ Alloying during Powder Bed Fusion of Metals Using a Laser Beam , Metals. 2021 Singh, A., Incardona, P. & Sbalzarini, I.F., A C++ expression system for partial differential equations enables generic simulations of biological hydrodynamics , The European Physical Journal E, 2021 Pic\u00f3, J., Vignoni, A., Boada, Y., Stochastic Differential Equations for Practical Simulation of Gene Circuits , Methods in Molecular Biology, 2021 Daniel R. Weilandt, Vassily Hatzimanikatis, Particle-Based Simulation Reveals Macromolecular Crowding Effects on the Michaelis-Menten Mechanism , Biophysical Journal, 2019 A. Gupta et al. A Proposed Framework for Interactive Virtual Reality In Situ Visualization of Parallel Numerical Simulations , 2019 IEEE 9th Symposium on Large Data Analysis and Visualization (LDAV), 2019 License BSD 3-Clause This project was supported in parts by the Deutsche Forschungsgemeinschaft (DFG), Germany under project \u2018\u2018OpenPME\u2019\u2019 and by the German Federal Ministry of Research and Education (BMBF), Germany under grant number 031L0044","title":"Cite us"},{"location":"about/#about-openfpm","text":"OpenFPM is an open-source C++ framework for parallel particles-only and hybrid particle-mesh codes. OpenFPM is intended as a successor to the discontinued PPM Library 1 , 2 . Please cite with: @article{INCARDONA2019155, title = {OpenFPM: A scalable open framework for particle and particle-mesh codes on parallel computers}, journal = {Computer Physics Communications}, volume = {241}, pages = {155-177}, year = {2019}, issn = {0010-4655}, doi = {https://doi.org/10.1016/j.cpc.2019.03.007}, url = {https://www.sciencedirect.com/science/article/pii/S0010465519300852}, author = {Pietro Incardona and Antonio Leo and Yaroslav Zaluzhnyi and Rajesh Ramaswamy and Ivo F. Sbalzarini},","title":"About OpenFPM"},{"location":"about/#other-publications-utilizing-openfpm","text":"Wimmer A, Panzer H, Zoeller C, Adami A, Adams N A & Zaeh M F, Experimental and numerical investigations of the hot cracking susceptibility during the powder bed fusion of AA 7075 using a laser beam , Progress in Additive Manufacturing, 2023 A. Singh, P. H. Suhrcke, P. Incardona, I. F. Sbalzarini, A numerical solver for active hydrodynamics in three dimensions and its application to active turbulence , Physics of Fluids, 2023 Singh, A., Foggia, A., Incardona, P. et al, A Meshfree Collocation Scheme for Surface Differential Operators on Point Clouds , Journal of Scientific Computing, 2023 Incardona P, Gupta A, Yaskovets S, Sbalzarini IF, A portable C++ library for memory and compute abstraction on multi-core CPUs and GPUs , Concurrency and Computation Practice and Experience 2023 C. Z\u00f6ller, N.A. Adams, S. Adami, Numerical investigation of balling defects in laser-based powder bed fusion of metals with Inconel 718 , Additive Manufacturing, 2023 Geara, S., Martin, S., Adami, S. et al. SPH 3D simulation of jet break-up driven by external vibrations , Cumputational Particle Mechanics, 2023 Singh, Abhinav and Vagne, Quentin and Julicher, Frank and Sbalzarini, Ivo F, Spontaneous flow instabilities of active polar fluids in three dimensions , Physical Review Research, 2023 Veettil, Sachin Krishnan Thekke and Zavalani, Gentian and Acosta, Uwe Hernandez and Sbalzarini, Ivo F. and Hecht, Michael, Global Polynomial Level Sets for Numerical Differential Geometry of Smooth Closed Surfaces , SIAM Journal on Scientific Computing, 2023 C. Z\u00f6ller, N.A. Adams, S. Adami, A partitioned continuous surface stress model for multiphase smoothed particle hydrodynamics , Journal of Computational Physics, 2023 Maddu Suryanarayana, Cheeseman Bevan L., Sbalzarini Ivo F. and M\u00fcller Christian L., Stability selection enables robust learning of differential equations from limited noisy data , Proceedings Of The Royal Society A Stoyanovskaya, O.P., Grigoryev, V.V., Suslenkova, A.N., Davydov, M.N., Snytnikov, N.V., Two-Phase Gas and Dust Free Expansion: Three-Dimensional Benchmark Problem for CFD Codes , Fluids 2022 Nesrine Khouzami, Friedrich Michel, Pietro Incardona, Jeronimo Castrillon, Ivo F. Sbalzarini, Model-based autotuning of discretization methods in numerical simulations of partial differential equations , Journal of Computational Science, 2022 Wimmer A, Yalvac B, Zoeller C, Hofstaetter F, Adami S, Adams NA, Zaeh MF., Experimental and Numerical Investigations of In Situ Alloying during Powder Bed Fusion of Metals Using a Laser Beam , Metals. 2021 Singh, A., Incardona, P. & Sbalzarini, I.F., A C++ expression system for partial differential equations enables generic simulations of biological hydrodynamics , The European Physical Journal E, 2021 Pic\u00f3, J., Vignoni, A., Boada, Y., Stochastic Differential Equations for Practical Simulation of Gene Circuits , Methods in Molecular Biology, 2021 Daniel R. Weilandt, Vassily Hatzimanikatis, Particle-Based Simulation Reveals Macromolecular Crowding Effects on the Michaelis-Menten Mechanism , Biophysical Journal, 2019 A. Gupta et al. A Proposed Framework for Interactive Virtual Reality In Situ Visualization of Parallel Numerical Simulations , 2019 IEEE 9th Symposium on Large Data Analysis and Visualization (LDAV), 2019","title":"Other publications utilizing OpenFPM"},{"location":"about/#license","text":"BSD 3-Clause This project was supported in parts by the Deutsche Forschungsgemeinschaft (DFG), Germany under project \u2018\u2018OpenPME\u2019\u2019 and by the German Federal Ministry of Research and Education (BMBF), Germany under grant number 031L0044","title":"License"},{"location":"building/","text":"Building from source List of Supported OS OS Architecture Support Linux x86-64/arm64 Yes/No macOS x86-64/arm64 Yes/Yes FreeBSD x86-64/arm64 No/No Windows (*only with Cygwin) x86-64/arm64 Yes/No Git Subprojects OpenFPM project includes the following subprojects Submodule Purpose Depends on openfpm_devices Memory management, GPU primitives openfpm_data Serial data structures openfpm_devices openfpm_vcluster Parallel communication openfpm_data openfpm_io Serial/Parallel Input-Output openfpm_data, openfpm_vcluster openfpm_pdata Parallel data structures openfpm_devices, openfpm_data, openfpm_vcluster, openfpm_io openfpm_numerics Numerical algorithms openfpm_data, openfpm_vcluster, openfpm_io, openfpm_pdata The subprojects are managed using Git submodules . Please refer to this manual on how to use this tool: git clone https://github.com/mosaic-group/openfpm cd openfpm git submodule init git submodule update # optional: switch to a non-master branch, e.g. develop git checkout develop git submodule foreach \"git checkout develop\" git submodule foreach \"git pull origin develop\" Dependencies Install prerequisites # for linux-based systems apt-get install build-essential make cmake make cmake git bzip2 libbz2-dev python-dev wget # or apt-get install gcc g++ gfortran libtool libxml2-dev libxslt-dev make cmake git bzip2 libbz2-dev python-dev wget # for other systems yum install g++ gcc-gfortran libtool make cmake git bzip2 bzip2-devel python-devel libxml2-devel libxslt-devel wget brew install gcc49 gcc-gfortran libtool make cmake git python bzip2 wget OpenFPM is build upon the following open-source tools. Please intall these by building from source or with a package manager. Building dependencies from source Tool Submodule Description Optional OpenFPM Version Open MPI openfpm_vcluster The Open MPI Project is an open source Message Passing Interface implementation NO 4.1.6 (building from source on cygwin not supported, has to be preinstalled) METIS openfpm_pdata METIS is a set of serial programs for partitioning graphs and producing fill reducing orderings for sparse matrices YES (or ParMETIS) 5.1.0 ParMETIS openfpm_pdata, openfpm_numerics Extends the functionality of METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations YES (or METIS) 4.0.3 BOOST openfpm_data, openfpm_vcluster, openfpm_io, openfpm_pdata, openfpm_numerics Set of libraries that provides support for templated data structures, multithreading, unit testing etc. NO 1.84.0 (*works on arm64 macOS only with clang ) zlib openfpm_io Lossless data-compression library needed by HDF5 to deflate stored files NO 1.3.1 (*doesn't work on Windows) HDF5 openfpm_io Distributed file format that supports large, complex, heterogeneous data. Requires zlib NO 1.14.3 Vc openfpm_data The library is a collection of SIMD vector classes with existing implementations for SSE, AVX, and a scalar fallback NO 1.4.4 libhilbert openfpm_data Library producing Hilbert indices for multidimensional data to iterate through the grid elements following an Hilbert space filling curve. NO master (*no active support) HIP openfpm_devices A C++ runtime API and kernel language that allows developers to create portable applications for AMD and NVIDIA GPUs from single source code. One of the alternative execution backends for CUDA-like code supported by OpenFPM Yes alpaka openfpm_devices A header-only C++17 abstraction library for accelerator development. One of the alternative execution backends for CUDA-like code supported by OpenFPM Yes OpenBLAS openfpm_numerics An optimized BLAS (Basic Linear Algebra Subprograms) library, used for performing basic vector and matrix operations NO* 0.3.26 suitesparse openfpm_numerics A suite of sparse matrix algorithms. Here UMFPACK - multifrontal LU factorization module. Requires [OpenBLAS](http://www.openblas.net/] NO* 5.7.2 Eigen openfpm_numerics Template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Requires suitesparse Yes* (or Petsc) 3.4.0 Blitz++ openfpm_numerics A meta-template library for array manipulation in C++ with a speed comparable to Fortran implementations, while preserving an object-oriented interface NO* 1.0.2 Algoim openfpm_numerics A collection of high-order accurate numerical methods and C++ algorithms for working with implicitly-defined geometry and level set methods. Requires Blitz++ NO* master PETSc openfpm_numerics Scientific computation toolkit for linear and non-linear solvers, preconditioners, time integrators. Installs HYPRE, MUMPS, ScaLAPACK, SuperLU_DIST. Requires OpenBLAS , suitesparse , ParMETIS Yes* (or Eigen) 3.19.6 *optional if openfpm_numerics is disabled OpenFPM uses GPUDirect RDMA to move data from one GPU to another GPU (intranode and extranode) without moving the data to host. This feature requires that OpenMPI is compiled with CUDA support. It can without this feature as well using GPUDirect of an old version (1.0). In practice it requires that MPI works with host pinned memory allocated with CUDA. This feature has been introduced with CUDA 5.0 in 2010. At best of our knowledge this feature should work without special compilation options for OpenMPI. On the other end we found several problems with GPUDirect v1.0 and Infiniband cards, when OpenMPI is not compiled with CUDA support. If you are on a super-computer or a machine you did not set-up, we suggest to re-install OpenMPI with CUDA support using the options suggested. Alternatively you can use the OpenMPI already provied. The following script installs OpenFPM dependencies to the directory /home/test/openfpm_dependencies , compiled libraries and headers of OpenFPM to /home/test/openfpm_install , uses gcc toolkit, 4 cores and no gpu support for OpenMPI. # here for gcc/g++. Also for icc/icpc and clang/clang++ # to enable gpu accelerated code in MPI installed, set GPU_CUDA_SUPPORT=1 export CC=gcc \\ CXX=g++ \\ F77=gfortran \\ FC=gfortran \\ PREFIX_DEPENDS=/home/test/openfpm_dependencies \\ PREFIX_OPENFPM=/home/test/openfpm_install \\ NCORE=4 \\ GPU_CUDA_SUPPORT=0 ./script/install_MPI.sh $PREFIX_DEPENDS $NCORE $GPU_CUDA_SUPPORT $CC $CXX $F77 $FC \"--with-mpivendor=openmpi\" export PATH=\"$PREFIX_DEPENDS/MPI/bin:$PATH\" ./script/install_Metis.sh $PREFIX_DEPENDS $NCORE $CC $CXX # Parmetis uses mpicc in make config by default ./script/install_Parmetis.sh $PREFIX_DEPENDS $NCORE ./script/install_BOOST.sh $PREFIX_DEPENDS $NCORE ./script/install_ZLIB.sh $PREFIX_DEPENDS $NCORE ./script/install_HDF5.sh $PREFIX_DEPENDS $NCORE # Zlib uses CC=mpicc in ./configure ./script/install_LIBHILBERT.sh $PREFIX_DEPENDS $NCORE ./script/install_VCDEVEL.sh $PREFIX_DEPENDS $NCORE $CC $CXX # Install dependencies for numerics (optional) ./script/install_OPENBLAS.sh $PREFIX_DEPENDS $NCORE ./script/install_SUITESPARSE.sh $PREFIX_DEPENDS $NCORE ./script/install_EIGEN.sh $PREFIX_DEPENDS $NCORE ./script/install_BLITZ.sh $PREFIX_DEPENDS $NCORE ./script/install_ALGOIM.sh $PREFIX_DEPENDS $NCORE ./script/install_PETSC.sh $PREFIX_DEPENDS $NCORE $CC $CXX $F77 $FC Building OpenFPM OpenFPM uses CMake build system. For it to function properly, CMake has to be able to locate the dependencies of OpenFPM. If they are not installed system-wide, the following script passes their locations to CMake. Additionaly, the script assumes some openfpm build parameters by default, e.g. build type, backend for CUDA-like code, enable/disable numerics module, debug utilities etc. ./script/conf_CMake.sh $PREFIX_DEPENDS $PREFIX_OPENFPM The resultant CMake command is echoed to the terminal window and saved into the file cmake_build_options . CMake config options are exported to header files to be used in the project mkdir -p openfpm_pdata/scr/config openfpm_devices/scr/config openfpm_io/scr/config openfpm_data/scr/config openfpm_numerics/scr/config openfpm_vcluster/scr/config mkdir build cd build #insert the output of conf_CMake.sh and run the command # <INSERT HERE> make VERBOSE=1 -j $NCORE make install cd .. If the dependencies are not installed system-wide, but build from source, two environment variables have to be set accordingly: LD_LIBRARY_PATH so the dynamic link loader knows where to search for the dynamic shared libraries; PATH so the binary files could be executed without specifying the full path, e.q. mpic++ This could be done manually (e.g. by modifying ~/.bashrc , ~/.zshrc ...) or with the following tool that produces the file open fpm_vars . This file has to be sourced every time in a new session before running OpenFPM related code. ./script/create_env_vars.sh $PREFIX_DEPENDS $PREFIX_OPENFPM source openfpm_vars Running Tests and Examples Optionally, all tests could be run in each module to assure the project and dependencies work correctly # openfpm_data cd openfpm_data mpirun -np 1 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/openfpm_data/src/mem_map --run_test=*/* --log_level=test_suite cd .. # openfpm_devices cd openfpm_devices mpirun -np 1 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/openfpm_data/openfpm_devices/src/mem --run_test=*/* --log_level=test_suite cd .. # openfpm_io cd openfpm_io mpirun -np 1 ../build/openfpm_pdata/openfpm_io/src/io --run_test=*/* --log_level=test_suite cd .. # openfpm_numerics cd openfpm_numerics mpirun -np 3 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/openfpm_data/openfpm_devices/openfpm_numerics/src/numerics --run_test=*/* --log_level=test_suite cd .. # openfpm_vcluter cd openfpm_vcluster mpirun -np 3 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/src/vcluster_test --run_test=*/* --log_level=test_suite cd .. # openfpm_pdata cd openfpm_pdata mpirun -np 3 ../build/openfpm_pdata/src/pdata --run_test=*/* --log_level=test_suite cd .. Example simulation codes are compiled with Makefile. For the dependencies (that are not installed system-wide) to be linked properly, the following compiler options have to be set: -llib Search for the library named lib when linking -Lloc The location loc where to search for lib -Idir Add the directory dir to the list of directories to be searched for header files during preprocessing. This could be done manually when compiling the example codes or with the following tool that produces the file example.mk . Warning : OpenFPM installed and the gpu examples have to be compiled in the same mode (i.e. via nvcc, hip, alpaka or gpu-emulated (SEQUENTIAL, OpenMP)) The file has to be placed in the folder example . ./script/create_example.mk.sh $PREFIX_DEPENDS $PREFIX_OPENFPM mv example.mk example In addition to the building from source described below, OpenFPM packages are also available as pre-built binaries and Docker images","title":"Build from source"},{"location":"building/#building-from-source","text":"","title":"Building from source"},{"location":"building/#list-of-supported-os","text":"OS Architecture Support Linux x86-64/arm64 Yes/No macOS x86-64/arm64 Yes/Yes FreeBSD x86-64/arm64 No/No Windows (*only with Cygwin) x86-64/arm64 Yes/No","title":"List of Supported OS"},{"location":"building/#git-subprojects","text":"OpenFPM project includes the following subprojects Submodule Purpose Depends on openfpm_devices Memory management, GPU primitives openfpm_data Serial data structures openfpm_devices openfpm_vcluster Parallel communication openfpm_data openfpm_io Serial/Parallel Input-Output openfpm_data, openfpm_vcluster openfpm_pdata Parallel data structures openfpm_devices, openfpm_data, openfpm_vcluster, openfpm_io openfpm_numerics Numerical algorithms openfpm_data, openfpm_vcluster, openfpm_io, openfpm_pdata The subprojects are managed using Git submodules . Please refer to this manual on how to use this tool: git clone https://github.com/mosaic-group/openfpm cd openfpm git submodule init git submodule update # optional: switch to a non-master branch, e.g. develop git checkout develop git submodule foreach \"git checkout develop\" git submodule foreach \"git pull origin develop\"","title":"Git Subprojects"},{"location":"building/#dependencies","text":"","title":"Dependencies"},{"location":"building/#install-prerequisites","text":"# for linux-based systems apt-get install build-essential make cmake make cmake git bzip2 libbz2-dev python-dev wget # or apt-get install gcc g++ gfortran libtool libxml2-dev libxslt-dev make cmake git bzip2 libbz2-dev python-dev wget # for other systems yum install g++ gcc-gfortran libtool make cmake git bzip2 bzip2-devel python-devel libxml2-devel libxslt-devel wget brew install gcc49 gcc-gfortran libtool make cmake git python bzip2 wget OpenFPM is build upon the following open-source tools. Please intall these by building from source or with a package manager.","title":"Install prerequisites"},{"location":"building/#building-dependencies-from-source","text":"Tool Submodule Description Optional OpenFPM Version Open MPI openfpm_vcluster The Open MPI Project is an open source Message Passing Interface implementation NO 4.1.6 (building from source on cygwin not supported, has to be preinstalled) METIS openfpm_pdata METIS is a set of serial programs for partitioning graphs and producing fill reducing orderings for sparse matrices YES (or ParMETIS) 5.1.0 ParMETIS openfpm_pdata, openfpm_numerics Extends the functionality of METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations YES (or METIS) 4.0.3 BOOST openfpm_data, openfpm_vcluster, openfpm_io, openfpm_pdata, openfpm_numerics Set of libraries that provides support for templated data structures, multithreading, unit testing etc. NO 1.84.0 (*works on arm64 macOS only with clang ) zlib openfpm_io Lossless data-compression library needed by HDF5 to deflate stored files NO 1.3.1 (*doesn't work on Windows) HDF5 openfpm_io Distributed file format that supports large, complex, heterogeneous data. Requires zlib NO 1.14.3 Vc openfpm_data The library is a collection of SIMD vector classes with existing implementations for SSE, AVX, and a scalar fallback NO 1.4.4 libhilbert openfpm_data Library producing Hilbert indices for multidimensional data to iterate through the grid elements following an Hilbert space filling curve. NO master (*no active support) HIP openfpm_devices A C++ runtime API and kernel language that allows developers to create portable applications for AMD and NVIDIA GPUs from single source code. One of the alternative execution backends for CUDA-like code supported by OpenFPM Yes alpaka openfpm_devices A header-only C++17 abstraction library for accelerator development. One of the alternative execution backends for CUDA-like code supported by OpenFPM Yes OpenBLAS openfpm_numerics An optimized BLAS (Basic Linear Algebra Subprograms) library, used for performing basic vector and matrix operations NO* 0.3.26 suitesparse openfpm_numerics A suite of sparse matrix algorithms. Here UMFPACK - multifrontal LU factorization module. Requires [OpenBLAS](http://www.openblas.net/] NO* 5.7.2 Eigen openfpm_numerics Template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Requires suitesparse Yes* (or Petsc) 3.4.0 Blitz++ openfpm_numerics A meta-template library for array manipulation in C++ with a speed comparable to Fortran implementations, while preserving an object-oriented interface NO* 1.0.2 Algoim openfpm_numerics A collection of high-order accurate numerical methods and C++ algorithms for working with implicitly-defined geometry and level set methods. Requires Blitz++ NO* master PETSc openfpm_numerics Scientific computation toolkit for linear and non-linear solvers, preconditioners, time integrators. Installs HYPRE, MUMPS, ScaLAPACK, SuperLU_DIST. Requires OpenBLAS , suitesparse , ParMETIS Yes* (or Eigen) 3.19.6 *optional if openfpm_numerics is disabled OpenFPM uses GPUDirect RDMA to move data from one GPU to another GPU (intranode and extranode) without moving the data to host. This feature requires that OpenMPI is compiled with CUDA support. It can without this feature as well using GPUDirect of an old version (1.0). In practice it requires that MPI works with host pinned memory allocated with CUDA. This feature has been introduced with CUDA 5.0 in 2010. At best of our knowledge this feature should work without special compilation options for OpenMPI. On the other end we found several problems with GPUDirect v1.0 and Infiniband cards, when OpenMPI is not compiled with CUDA support. If you are on a super-computer or a machine you did not set-up, we suggest to re-install OpenMPI with CUDA support using the options suggested. Alternatively you can use the OpenMPI already provied. The following script installs OpenFPM dependencies to the directory /home/test/openfpm_dependencies , compiled libraries and headers of OpenFPM to /home/test/openfpm_install , uses gcc toolkit, 4 cores and no gpu support for OpenMPI. # here for gcc/g++. Also for icc/icpc and clang/clang++ # to enable gpu accelerated code in MPI installed, set GPU_CUDA_SUPPORT=1 export CC=gcc \\ CXX=g++ \\ F77=gfortran \\ FC=gfortran \\ PREFIX_DEPENDS=/home/test/openfpm_dependencies \\ PREFIX_OPENFPM=/home/test/openfpm_install \\ NCORE=4 \\ GPU_CUDA_SUPPORT=0 ./script/install_MPI.sh $PREFIX_DEPENDS $NCORE $GPU_CUDA_SUPPORT $CC $CXX $F77 $FC \"--with-mpivendor=openmpi\" export PATH=\"$PREFIX_DEPENDS/MPI/bin:$PATH\" ./script/install_Metis.sh $PREFIX_DEPENDS $NCORE $CC $CXX # Parmetis uses mpicc in make config by default ./script/install_Parmetis.sh $PREFIX_DEPENDS $NCORE ./script/install_BOOST.sh $PREFIX_DEPENDS $NCORE ./script/install_ZLIB.sh $PREFIX_DEPENDS $NCORE ./script/install_HDF5.sh $PREFIX_DEPENDS $NCORE # Zlib uses CC=mpicc in ./configure ./script/install_LIBHILBERT.sh $PREFIX_DEPENDS $NCORE ./script/install_VCDEVEL.sh $PREFIX_DEPENDS $NCORE $CC $CXX # Install dependencies for numerics (optional) ./script/install_OPENBLAS.sh $PREFIX_DEPENDS $NCORE ./script/install_SUITESPARSE.sh $PREFIX_DEPENDS $NCORE ./script/install_EIGEN.sh $PREFIX_DEPENDS $NCORE ./script/install_BLITZ.sh $PREFIX_DEPENDS $NCORE ./script/install_ALGOIM.sh $PREFIX_DEPENDS $NCORE ./script/install_PETSC.sh $PREFIX_DEPENDS $NCORE $CC $CXX $F77 $FC","title":"Building dependencies from source"},{"location":"building/#building-openfpm","text":"OpenFPM uses CMake build system. For it to function properly, CMake has to be able to locate the dependencies of OpenFPM. If they are not installed system-wide, the following script passes their locations to CMake. Additionaly, the script assumes some openfpm build parameters by default, e.g. build type, backend for CUDA-like code, enable/disable numerics module, debug utilities etc. ./script/conf_CMake.sh $PREFIX_DEPENDS $PREFIX_OPENFPM The resultant CMake command is echoed to the terminal window and saved into the file cmake_build_options . CMake config options are exported to header files to be used in the project mkdir -p openfpm_pdata/scr/config openfpm_devices/scr/config openfpm_io/scr/config openfpm_data/scr/config openfpm_numerics/scr/config openfpm_vcluster/scr/config mkdir build cd build #insert the output of conf_CMake.sh and run the command # <INSERT HERE> make VERBOSE=1 -j $NCORE make install cd .. If the dependencies are not installed system-wide, but build from source, two environment variables have to be set accordingly: LD_LIBRARY_PATH so the dynamic link loader knows where to search for the dynamic shared libraries; PATH so the binary files could be executed without specifying the full path, e.q. mpic++ This could be done manually (e.g. by modifying ~/.bashrc , ~/.zshrc ...) or with the following tool that produces the file open fpm_vars . This file has to be sourced every time in a new session before running OpenFPM related code. ./script/create_env_vars.sh $PREFIX_DEPENDS $PREFIX_OPENFPM source openfpm_vars","title":"Building OpenFPM"},{"location":"building/#running-tests-and-examples","text":"Optionally, all tests could be run in each module to assure the project and dependencies work correctly # openfpm_data cd openfpm_data mpirun -np 1 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/openfpm_data/src/mem_map --run_test=*/* --log_level=test_suite cd .. # openfpm_devices cd openfpm_devices mpirun -np 1 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/openfpm_data/openfpm_devices/src/mem --run_test=*/* --log_level=test_suite cd .. # openfpm_io cd openfpm_io mpirun -np 1 ../build/openfpm_pdata/openfpm_io/src/io --run_test=*/* --log_level=test_suite cd .. # openfpm_numerics cd openfpm_numerics mpirun -np 3 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/openfpm_data/openfpm_devices/openfpm_numerics/src/numerics --run_test=*/* --log_level=test_suite cd .. # openfpm_vcluter cd openfpm_vcluster mpirun -np 3 ../build/openfpm_pdata/openfpm_io/openfpm_vcluster/src/vcluster_test --run_test=*/* --log_level=test_suite cd .. # openfpm_pdata cd openfpm_pdata mpirun -np 3 ../build/openfpm_pdata/src/pdata --run_test=*/* --log_level=test_suite cd .. Example simulation codes are compiled with Makefile. For the dependencies (that are not installed system-wide) to be linked properly, the following compiler options have to be set: -llib Search for the library named lib when linking -Lloc The location loc where to search for lib -Idir Add the directory dir to the list of directories to be searched for header files during preprocessing. This could be done manually when compiling the example codes or with the following tool that produces the file example.mk . Warning : OpenFPM installed and the gpu examples have to be compiled in the same mode (i.e. via nvcc, hip, alpaka or gpu-emulated (SEQUENTIAL, OpenMP)) The file has to be placed in the folder example . ./script/create_example.mk.sh $PREFIX_DEPENDS $PREFIX_OPENFPM mv example.mk example In addition to the building from source described below, OpenFPM packages are also available as pre-built binaries and Docker images","title":"Running Tests and Examples"},{"location":"docker/","text":"Installation from Docker Containers OpenFPM provides Docker containers with OpenFPM installed. Docker is an open platform for developers to build, ship, and run distributed applications. Please refer to user manuals to find more information about this tool, e.g. here OpenFPM provides the following CPU-only docker images with OpenFPM pre-installed openfpm/ubuntu:install20.04 , openfpm/fedora:install34 . Or the images with the GPU code enabled openfpm/ubuntu_cuda:install10.2-devel-ubuntu18.04 , openfpm/ubuntu_cuda:install11.2.2-devel-ubuntu20.04 . To start the container in Linux general command would be [sudo] docker run --net=host \\ -dit -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY=unix$DISPLAY \\ -v $HOME/<shared host folder>:<shared container folder> \\ -e GDK_SCALE -e GDK_DPI_SCALE \\ --gpus all `#for gpu containers, requires nvidia-container-toolkit` \\ --name openfpm openfpm/ubuntu:install20.04 bash where the networking of the container is not containerized, but connected to the host network X11 forwarding enabled enabled folder sharing between the host and the container file systems all GPU resources are shared between the host and the container To connect to the running container use [sudo] docker exec -it openfpm /bin/bash Inside the container the preinstalled dependencies are located in /root , the file with environment variables openfpm_vars is located is /root , the source code of OpenFPM located in /openfpm/openfpm_pdata , installation files of OpenFPM are located in /usr/local .","title":"Docker"},{"location":"docker/#installation-from-docker-containers","text":"OpenFPM provides Docker containers with OpenFPM installed. Docker is an open platform for developers to build, ship, and run distributed applications. Please refer to user manuals to find more information about this tool, e.g. here OpenFPM provides the following CPU-only docker images with OpenFPM pre-installed openfpm/ubuntu:install20.04 , openfpm/fedora:install34 . Or the images with the GPU code enabled openfpm/ubuntu_cuda:install10.2-devel-ubuntu18.04 , openfpm/ubuntu_cuda:install11.2.2-devel-ubuntu20.04 . To start the container in Linux general command would be [sudo] docker run --net=host \\ -dit -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY=unix$DISPLAY \\ -v $HOME/<shared host folder>:<shared container folder> \\ -e GDK_SCALE -e GDK_DPI_SCALE \\ --gpus all `#for gpu containers, requires nvidia-container-toolkit` \\ --name openfpm openfpm/ubuntu:install20.04 bash where the networking of the container is not containerized, but connected to the host network X11 forwarding enabled enabled folder sharing between the host and the container file systems all GPU resources are shared between the host and the container To connect to the running container use [sudo] docker exec -it openfpm /bin/bash Inside the container the preinstalled dependencies are located in /root , the file with environment variables openfpm_vars is located is /root , the source code of OpenFPM located in /openfpm/openfpm_pdata , installation files of OpenFPM are located in /usr/local .","title":"Installation from Docker Containers"},{"location":"download/","text":"All Releases Release Date Size Version Filename Mar 29 2022 87M 4.1.0 openfpm_pdata-4.1.0-Linux.15.3.x86_64.rpm Mar 29 2022 135M 4.1.0 openfpm_pdata-4.1.0-Linux.20-amd64.x86_64.deb Mar 29 2022 115M 4.1.0 OpenFPM-4.1.0-Darwin.bigsur-x86.x86_64.pkg Mar 29 2022 94M 4.1.0 OpenFPM-4.1.0-Darwin.bigsur-x86.aarch64.pkg Mar 29 2022 87M 4.1.0 openfpm_pdata-4.1.0-Linux.15.2.x86_64.rpm Mar 29 2022 130M 4.1.0 openfpm_pdata-4.1.0-Linux.bullseye.x86_64.deb Mar 29 2022 126M 4.1.0 openfpm_pdata-4.1.0-Linux.stretch.x86_64.deb Mar 29 2022 132M 4.1.0 openfpm_pdata-4.1.0-Linux.buster.x86_64.deb Mar 29 2022 82M 4.1.0 openfpm_pdata-4.1.0-Linux.33.x86_64.rpm Mar 29 2022 82M 4.1.0 openfpm_pdata-4.1.0-Linux.34.x86_64.rpm Mar 29 2022 134M 4.1.0 openfpm_pdata-4.1.0-Linux.20.04.x86_64.deb Mar 29 2022 128M 4.1.0 openfpm_pdata-4.1.0-Linux.18.04.x86_64.deb Nov 15 2021 87M 4.0.0 openfpm_pdata-4.0.0-Linux.15.3.x86_64.rpm Nov 15 2021 135M 4.0.0 openfpm_pdata-4.0.0-Linux.20-amd64.x86_64.deb Nov 15 2021 87M 4.0.0 openfpm_pdata-4.0.0-Linux.15.2.x86_64.rpm Nov 15 2021 126M 4.0.0 openfpm_pdata-4.0.0-Linux.stretch.x86_64.deb Nov 15 2021 130M 4.0.0 openfpm_pdata-4.0.0-Linux.bullseye.x86_64.deb Nov 15 2021 132M 4.0.0 openfpm_pdata-4.0.0-Linux.buster.x86_64.deb Nov 15 2021 82M 4.0.0 openfpm_pdata-4.0.0-Linux.33.x86_64.rpm Nov 15 2021 82M 4.0.0 openfpm_pdata-4.0.0-Linux.34.x86_64.rpm Nov 15 2021 135M 4.0.0 openfpm_pdata-4.0.0-Linux.20.04.x86_64.deb Nov 15 2021 128M 4.0.0 openfpm_pdata-4.0.0-Linux.18.04.x86_64.deb Nov 14 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-x86.x86_64.pkg Nov 14 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-x86.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.osx.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.osx.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-arm.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-arm.aarch64.pkg Sep 2 2021 133M 4.0.0 openfpm_pdata-4.0.0-Linux.base-devel-20210131.0.14634.x86_64.tar.gz Aug 30 2021 87M 3.3.0 openfpm_pdata-3.3.0-Linux.8.x86_64.rpm Aug 30 2021 86M 3.3.0 openfpm_pdata-3.3.0-Linux.15.3.x86_64.rpm Aug 30 2021 132M 3.3.0 openfpm_pdata-3.3.0-Linux.base-devel-20210131.0.14634.x86_64.tar.gz Aug 30 2021 114M 3.3.0 OpenFPM-3.3.0-Darwin.bigsur.x86_64.pkg Aug 30 2021 93M 3.3.0 OpenFPM-3.3.0-Darwin.bigsur.aarch64.pkg Aug 30 2021 86M 3.3.0 openfpm_pdata-3.3.0-Linux.15.2.x86_64.rpm Aug 30 2021 125M 3.3.0 openfpm_pdata-3.3.0-Linux.stretch.x86_64.deb Aug 30 2021 130M 3.3.0 openfpm_pdata-3.3.0-Linux.bullseye.x86_64.deb Aug 30 2021 132M 3.3.0 openfpm_pdata-3.3.0-Linux.buster.x86_64.deb Aug 30 2021 82M 3.3.0 openfpm_pdata-3.3.0-Linux.34.x86_64.rpm Aug 30 2021 82M 3.3.0 openfpm_pdata-3.3.0-Linux.33.x86_64.rpm Aug 30 2021 134M 3.3.0 openfpm_pdata-3.3.0-Linux.20.04.x86_64.deb Aug 30 2021 127M 3.3.0 openfpm_pdata-3.3.0-Linux.18.04.x86_64.deb May 9 2021 114M 3.3.0 OpenFPM-3.3.0-Darwin.catalina.x86_64.pkg Apr 30 2021 87M 3.2.0 openfpm_pdata-3.2.0-Linux.8.x86_64.rpm Apr 30 2021 86M 3.2.0 openfpm_pdata-3.2.0-Linux.15.3.x86_64.rpm Apr 30 2021 132M 3.2.0 openfpm_pdata-3.2.0-Linux.base-devel-20210131.0.14634.x86_64.tar.gz Apr 30 2021 86M 3.2.0 openfpm_pdata-3.2.0-Linux.15.2.x86_64.rpm Apr 30 2021 130M 3.2.0 openfpm_pdata-3.2.0-Linux.bullseye.x86_64.deb Apr 30 2021 125M 3.2.0 openfpm_pdata-3.2.0-Linux.stretch.x86_64.deb Apr 30 2021 132M 3.2.0 openfpm_pdata-3.2.0-Linux.buster.x86_64.deb Apr 30 2021 82M 3.2.0 openfpm_pdata-3.2.0-Linux.34.x86_64.rpm Apr 30 2021 82M 3.2.0 openfpm_pdata-3.2.0-Linux.33.x86_64.rpm Apr 30 2021 134M 3.2.0 openfpm_pdata-3.2.0-Linux.20.04.x86_64.deb Apr 30 2021 127M 3.2.0 openfpm_pdata-3.2.0-Linux.18.04.x86_64.deb Apr 28 2021 82M 3.2.0 openfpm_pdata-3.2.0-Linux.32.x86_64.rpm Apr 19 2021 117M 3.2.0 OpenFPM-3.2.0-Darwin.bigsur.x86_64.pkg Feb 28 2021 117M 3.2.0 OpenFPM-3.2.0-Darwin.catalina.x86_64.pkg Please use our GitHub issue tracker to report bugs or post questions or comments .","title":"Download"},{"location":"download/#all-releases","text":"Release Date Size Version Filename Mar 29 2022 87M 4.1.0 openfpm_pdata-4.1.0-Linux.15.3.x86_64.rpm Mar 29 2022 135M 4.1.0 openfpm_pdata-4.1.0-Linux.20-amd64.x86_64.deb Mar 29 2022 115M 4.1.0 OpenFPM-4.1.0-Darwin.bigsur-x86.x86_64.pkg Mar 29 2022 94M 4.1.0 OpenFPM-4.1.0-Darwin.bigsur-x86.aarch64.pkg Mar 29 2022 87M 4.1.0 openfpm_pdata-4.1.0-Linux.15.2.x86_64.rpm Mar 29 2022 130M 4.1.0 openfpm_pdata-4.1.0-Linux.bullseye.x86_64.deb Mar 29 2022 126M 4.1.0 openfpm_pdata-4.1.0-Linux.stretch.x86_64.deb Mar 29 2022 132M 4.1.0 openfpm_pdata-4.1.0-Linux.buster.x86_64.deb Mar 29 2022 82M 4.1.0 openfpm_pdata-4.1.0-Linux.33.x86_64.rpm Mar 29 2022 82M 4.1.0 openfpm_pdata-4.1.0-Linux.34.x86_64.rpm Mar 29 2022 134M 4.1.0 openfpm_pdata-4.1.0-Linux.20.04.x86_64.deb Mar 29 2022 128M 4.1.0 openfpm_pdata-4.1.0-Linux.18.04.x86_64.deb Nov 15 2021 87M 4.0.0 openfpm_pdata-4.0.0-Linux.15.3.x86_64.rpm Nov 15 2021 135M 4.0.0 openfpm_pdata-4.0.0-Linux.20-amd64.x86_64.deb Nov 15 2021 87M 4.0.0 openfpm_pdata-4.0.0-Linux.15.2.x86_64.rpm Nov 15 2021 126M 4.0.0 openfpm_pdata-4.0.0-Linux.stretch.x86_64.deb Nov 15 2021 130M 4.0.0 openfpm_pdata-4.0.0-Linux.bullseye.x86_64.deb Nov 15 2021 132M 4.0.0 openfpm_pdata-4.0.0-Linux.buster.x86_64.deb Nov 15 2021 82M 4.0.0 openfpm_pdata-4.0.0-Linux.33.x86_64.rpm Nov 15 2021 82M 4.0.0 openfpm_pdata-4.0.0-Linux.34.x86_64.rpm Nov 15 2021 135M 4.0.0 openfpm_pdata-4.0.0-Linux.20.04.x86_64.deb Nov 15 2021 128M 4.0.0 openfpm_pdata-4.0.0-Linux.18.04.x86_64.deb Nov 14 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-x86.x86_64.pkg Nov 14 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-x86.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.osx.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.osx.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.aarch64.pkg Sep 6 2021 115M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-arm.x86_64.pkg Sep 6 2021 94M 4.0.0 OpenFPM-4.0.0-Darwin.bigsur-arm.aarch64.pkg Sep 2 2021 133M 4.0.0 openfpm_pdata-4.0.0-Linux.base-devel-20210131.0.14634.x86_64.tar.gz Aug 30 2021 87M 3.3.0 openfpm_pdata-3.3.0-Linux.8.x86_64.rpm Aug 30 2021 86M 3.3.0 openfpm_pdata-3.3.0-Linux.15.3.x86_64.rpm Aug 30 2021 132M 3.3.0 openfpm_pdata-3.3.0-Linux.base-devel-20210131.0.14634.x86_64.tar.gz Aug 30 2021 114M 3.3.0 OpenFPM-3.3.0-Darwin.bigsur.x86_64.pkg Aug 30 2021 93M 3.3.0 OpenFPM-3.3.0-Darwin.bigsur.aarch64.pkg Aug 30 2021 86M 3.3.0 openfpm_pdata-3.3.0-Linux.15.2.x86_64.rpm Aug 30 2021 125M 3.3.0 openfpm_pdata-3.3.0-Linux.stretch.x86_64.deb Aug 30 2021 130M 3.3.0 openfpm_pdata-3.3.0-Linux.bullseye.x86_64.deb Aug 30 2021 132M 3.3.0 openfpm_pdata-3.3.0-Linux.buster.x86_64.deb Aug 30 2021 82M 3.3.0 openfpm_pdata-3.3.0-Linux.34.x86_64.rpm Aug 30 2021 82M 3.3.0 openfpm_pdata-3.3.0-Linux.33.x86_64.rpm Aug 30 2021 134M 3.3.0 openfpm_pdata-3.3.0-Linux.20.04.x86_64.deb Aug 30 2021 127M 3.3.0 openfpm_pdata-3.3.0-Linux.18.04.x86_64.deb May 9 2021 114M 3.3.0 OpenFPM-3.3.0-Darwin.catalina.x86_64.pkg Apr 30 2021 87M 3.2.0 openfpm_pdata-3.2.0-Linux.8.x86_64.rpm Apr 30 2021 86M 3.2.0 openfpm_pdata-3.2.0-Linux.15.3.x86_64.rpm Apr 30 2021 132M 3.2.0 openfpm_pdata-3.2.0-Linux.base-devel-20210131.0.14634.x86_64.tar.gz Apr 30 2021 86M 3.2.0 openfpm_pdata-3.2.0-Linux.15.2.x86_64.rpm Apr 30 2021 130M 3.2.0 openfpm_pdata-3.2.0-Linux.bullseye.x86_64.deb Apr 30 2021 125M 3.2.0 openfpm_pdata-3.2.0-Linux.stretch.x86_64.deb Apr 30 2021 132M 3.2.0 openfpm_pdata-3.2.0-Linux.buster.x86_64.deb Apr 30 2021 82M 3.2.0 openfpm_pdata-3.2.0-Linux.34.x86_64.rpm Apr 30 2021 82M 3.2.0 openfpm_pdata-3.2.0-Linux.33.x86_64.rpm Apr 30 2021 134M 3.2.0 openfpm_pdata-3.2.0-Linux.20.04.x86_64.deb Apr 30 2021 127M 3.2.0 openfpm_pdata-3.2.0-Linux.18.04.x86_64.deb Apr 28 2021 82M 3.2.0 openfpm_pdata-3.2.0-Linux.32.x86_64.rpm Apr 19 2021 117M 3.2.0 OpenFPM-3.2.0-Darwin.bigsur.x86_64.pkg Feb 28 2021 117M 3.2.0 OpenFPM-3.2.0-Darwin.catalina.x86_64.pkg Please use our GitHub issue tracker to report bugs or post questions or comments .","title":"All Releases"},{"location":"hackathon23/","text":"OpenFPM Hackathon: Winter 2023 (Archived) OpenFPM Hackathon We are pleased to announce the first OpenFPM Hackathon to take place February 9/10, 2023 at the Center for Systems Biology Dresden. With OpenFPM developing into a global open-source project for scalable scientific computing, the community desires to meet in order to synchronize contributions, review and produce code, and get to know the core developers personally. This 2-day in-person Hackathon is aimed at users, contributors, and developers. The core OpenFPM Developers will be present and available during the entire hackathon, so this is a great opportunity to get your ideas and wishes into the project and start close collaborations with them. But the Hackathon also welcomes people with little or no C++ coding skills who are willing to use the two days to contribute towards the OpenFPM documentation, tutorials, or revamping the OpenFPM web page . Additionally, we welcome potential new users and people who are interested in learning more about OpenFPM in order to decide whether to start using it. You will have all the expert users and developers on site to talk to and answer your questions, and you can implement a first own hands-on example implementation to get familiar with OpenFPM and its usage. But most importantly, this serves as a kick-off meeting for building a global community around OpenFPM. Be part of it! Time and Location: Location: Center for Systems Biology Dresden, Pfotenhauerstr. 108, D-01307 Dresden, Germany. Seminar Room Top Floor. For directions, please see here . Time: February 9 and 10, 2023 (2 days) with the following agenda: Day 1 - Feb 9, 2023: 09.00 - 12.30 Topic discussion, code review, division of tasks 12.30 - 13.30 Lunch break (self-paid) 13.30 - 16.30 Active coding 16.30 - 18.00 Problem review & feedback Day 2 - Feb 10, 2023: 09.00 - 12.00 Active coding 12.00 - 13.00 Lunch break (self-paid) 13.30 - 14.00 Preparing 10 min presentation 14.00 - 16.00 Plenary project presentation and discussion 16.00 - 17.30 Friday seminar & beer hour Travel logistics and fee: Participation in the hackathon is free of charge. There is no registration fee. Participants are expected to book and cover their travel arrangements by themselves. We can not provide financial or administrative support for travel to and from Dresden. Also, food during the hackathon is on self-pay basis. There is a canteen conveniently on site, and several affordable restaurant options in walking distance. Budget around 10 Euros for a meal. You can find directions to CSBD here . If you are arriving by train, both Dresden Main Station and Dresden Neustadt station are conveniently connected to our institute by tramway or bus. If you are arriving by plane, the easiest option is to look for flights into Dresden International Airport. Intercontinental connections need to transit in Zurich, Munich, or Frankfurt. From Dresden airport, there are trains to the city center (travel time 10 min) or you can take a taxi cab for around 30 Euros. Alternatively, the airports of Berlin (BER) and Prague are also feasible options. From there, buses and/or train service runs to Dresden in about 90 minutes. For accommodation, there are several hotels in walking distance from the institute, and many more in the city center with tramway service running to the institute. We can recommend the following hotels in the vicinity: Hotel am Blauen Wunder Hotel Andreas Please do not hesitate to get in touch with us if you have questions or require assistance with hotel booking. Registration: Everyone interested is welcome to attend, regardless if you are already an OpenFPM crack, a beginner, or not yet using it at all; regardless if you want to contribute code, learn for yourself, or work on documentation and web page. There is something for every skillset. If you wish to attend, please send an e-mail to the organizer Serhii Yaskovets , so we can have a tally and book rooms accordingly. This also applies to local applicants from CSBD itself or other Dresden institutions. Thanks! Registration deadline: Jan 31, 2023. Proposed topics Following are some ideas for topics people could work on. But of course, the ideal case is that participants come with their own topics and goals and take advantage of the presence of the OpenFPM Developers in order to get them realized and suggest changes or extensions to the library core. Feel free to e-mail your topic to the organizers ahead of time, or simply bring it along with you to the opening plenary! Importantly, there are also topics, like writing documentation or redoing the OpenFPM web page, that do not require C++ coding skills. In this sense, the hackathon also welcomes participants wishing to work on these important topics. Topic proposal collected so far: Granular contact models that would enable OpenFPM to perform granular simulations either of Hookean (linear) or Hertzian (non-linear) dynamics. Implement both approaches on top of OpenFPM data structures and implement a client application for DEM simulation of granular matter. Extend support for polynomial regression to n-dimensional data by adding a wrapper for the minter library. One can then use minter to obtain polynomial representations of data stored as OpenFPM grid or particle properties. Currently, there is a stub implementation only for particle properties. This should be extended to grids and subgrids with a uniform interface. Rewrite the OpenFPM web page to make it easier to navigate, more informative and up-to-date. Extend the existing primers and tutorials. Integrate with Read the Docs. Integrate and wrap in OpenFPM the Adaptive Particle Representation library ( libAPR ) for multi-resolution and adaptive simulations.","title":"Hackathon 2022/2023"},{"location":"hackathon23/#openfpm-hackathon-winter-2023-archived","text":"","title":"OpenFPM Hackathon: Winter 2023 (Archived)"},{"location":"hackathon23/#openfpm-hackathon","text":"We are pleased to announce the first OpenFPM Hackathon to take place February 9/10, 2023 at the Center for Systems Biology Dresden. With OpenFPM developing into a global open-source project for scalable scientific computing, the community desires to meet in order to synchronize contributions, review and produce code, and get to know the core developers personally. This 2-day in-person Hackathon is aimed at users, contributors, and developers. The core OpenFPM Developers will be present and available during the entire hackathon, so this is a great opportunity to get your ideas and wishes into the project and start close collaborations with them. But the Hackathon also welcomes people with little or no C++ coding skills who are willing to use the two days to contribute towards the OpenFPM documentation, tutorials, or revamping the OpenFPM web page . Additionally, we welcome potential new users and people who are interested in learning more about OpenFPM in order to decide whether to start using it. You will have all the expert users and developers on site to talk to and answer your questions, and you can implement a first own hands-on example implementation to get familiar with OpenFPM and its usage. But most importantly, this serves as a kick-off meeting for building a global community around OpenFPM. Be part of it!","title":"OpenFPM Hackathon"},{"location":"hackathon23/#time-and-location","text":"Location: Center for Systems Biology Dresden, Pfotenhauerstr. 108, D-01307 Dresden, Germany. Seminar Room Top Floor. For directions, please see here . Time: February 9 and 10, 2023 (2 days) with the following agenda: Day 1 - Feb 9, 2023: 09.00 - 12.30 Topic discussion, code review, division of tasks 12.30 - 13.30 Lunch break (self-paid) 13.30 - 16.30 Active coding 16.30 - 18.00 Problem review & feedback Day 2 - Feb 10, 2023: 09.00 - 12.00 Active coding 12.00 - 13.00 Lunch break (self-paid) 13.30 - 14.00 Preparing 10 min presentation 14.00 - 16.00 Plenary project presentation and discussion 16.00 - 17.30 Friday seminar & beer hour","title":"Time and Location:"},{"location":"hackathon23/#travel-logistics-and-fee","text":"Participation in the hackathon is free of charge. There is no registration fee. Participants are expected to book and cover their travel arrangements by themselves. We can not provide financial or administrative support for travel to and from Dresden. Also, food during the hackathon is on self-pay basis. There is a canteen conveniently on site, and several affordable restaurant options in walking distance. Budget around 10 Euros for a meal. You can find directions to CSBD here . If you are arriving by train, both Dresden Main Station and Dresden Neustadt station are conveniently connected to our institute by tramway or bus. If you are arriving by plane, the easiest option is to look for flights into Dresden International Airport. Intercontinental connections need to transit in Zurich, Munich, or Frankfurt. From Dresden airport, there are trains to the city center (travel time 10 min) or you can take a taxi cab for around 30 Euros. Alternatively, the airports of Berlin (BER) and Prague are also feasible options. From there, buses and/or train service runs to Dresden in about 90 minutes. For accommodation, there are several hotels in walking distance from the institute, and many more in the city center with tramway service running to the institute. We can recommend the following hotels in the vicinity: Hotel am Blauen Wunder Hotel Andreas Please do not hesitate to get in touch with us if you have questions or require assistance with hotel booking.","title":"Travel logistics and fee:"},{"location":"hackathon23/#registration","text":"Everyone interested is welcome to attend, regardless if you are already an OpenFPM crack, a beginner, or not yet using it at all; regardless if you want to contribute code, learn for yourself, or work on documentation and web page. There is something for every skillset. If you wish to attend, please send an e-mail to the organizer Serhii Yaskovets , so we can have a tally and book rooms accordingly. This also applies to local applicants from CSBD itself or other Dresden institutions. Thanks! Registration deadline: Jan 31, 2023.","title":"Registration:"},{"location":"hackathon23/#proposed-topics","text":"Following are some ideas for topics people could work on. But of course, the ideal case is that participants come with their own topics and goals and take advantage of the presence of the OpenFPM Developers in order to get them realized and suggest changes or extensions to the library core. Feel free to e-mail your topic to the organizers ahead of time, or simply bring it along with you to the opening plenary! Importantly, there are also topics, like writing documentation or redoing the OpenFPM web page, that do not require C++ coding skills. In this sense, the hackathon also welcomes participants wishing to work on these important topics. Topic proposal collected so far: Granular contact models that would enable OpenFPM to perform granular simulations either of Hookean (linear) or Hertzian (non-linear) dynamics. Implement both approaches on top of OpenFPM data structures and implement a client application for DEM simulation of granular matter. Extend support for polynomial regression to n-dimensional data by adding a wrapper for the minter library. One can then use minter to obtain polynomial representations of data stored as OpenFPM grid or particle properties. Currently, there is a stub implementation only for particle properties. This should be extended to grids and subgrids with a uniform interface. Rewrite the OpenFPM web page to make it easier to navigate, more informative and up-to-date. Extend the existing primers and tutorials. Integrate with Read the Docs. Integrate and wrap in OpenFPM the Adaptive Particle Representation library ( libAPR ) for multi-resolution and adaptive simulations.","title":"Proposed topics"},{"location":"hackathon24/","text":"OpenFPM Hackathon: Spring 2024 OpenFPM Hackathon We are pleased to announce OpenFPM Hackathon 2024 to take place March 4 - March 8, 2024 in Munich. With OpenFPM developing into a global open-source project for scalable scientific computing, the community desires to meet in order to synchronize contributions, review and produce code, and get to know the core developers personally. This year we have 5-day in-person Hackathon, which is aimed at users, contributors, and developers. The core OpenFPM Developers will be present and available during the entire hackathon, so this is a great opportunity to get your ideas and wishes into the project and start close collaborations with them. Additionally, we welcome potential new users and people who are interested in learning more about OpenFPM in order to decide whether to start using it. You will have all the expert users and developers on site to talk to and answer your questions, and you can implement a first own hands-on example implementation to get familiar with OpenFPM and its usage. Last year's event made multiple algorithmic and computational ideas come true. Be part of it in 2024! Location Munich Institute of Integrated Materials, Energy and Process Engineering Seminar-Room Ground floor Technische Universit\u00e4t M\u00fcnchen Lichtenbergstr. 4a 85748 Garching Timeplan (details subject to change): Day 1 - March 4: 09.00 - 12.30 Introduction to Openfpm: architecture, paradigms of development, application use-cases 12.30 - 13.30 Lunch break 13.30 - 15.00 Discussion: future of OpenFPM - its target audience, missing modules/features, ways to improve 15.00 - 16.00 Open problems: code review 16.00 - 18.00 Discussion: current issues and ways to solve them Day 2 - March 5: 09.00 - 12.30 Open problems: coding session 1 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: coding session 2 16.30 - 18.00 Discussion: sustainability of OpenFPM in the realm of scientific software Day 3 - March 6: 09.00 - 12.30 Open problems: coding session 1 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: coding session 2 16.30 - 18.00 Discussion: the art of reproducibility - best practices Day 4 - March 7: 09.00 - 12.30 Open problems: coding session 1 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: coding session 2 16.30 - 18.00 Discussion: integration with GitHub Day 5 - March 8 09.00 - 12.30 Open problems: wrap-up session 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: plenary presentation & discussion Travel logistics and fee: Participation in the hackathon is free of charge. There is no registration fee. Participants are expected to book and cover their travel arrangements by themselves. We can not provide financial or administrative support for travel to and from Munich. Also, food during the hackathon is on self-pay basis. Please find more information about Munich public transport network here . We can recommend the following hotels in the vicinity: Stellaris Apartment Hotel , on-campus. Booking codes for discounts at this place are available upon registration (see below) Hotel Marriott , on-campus. Booking codes for discounts at this place are available upon registration (see below) Hotel Koenig Ludwig II , Garching (off-campus), 5 min. Public transport to the campus Please do not hesitate to get in touch with us if you have questions or require assistance with hotel booking. Registration: Everyone interested is welcome to attend, regardless if you are already an OpenFPM crack, a beginner, or not yet using it at all; regardless if you want to contribute code, learn for yourself, or work on documentation and web page. There is something for every skillset. If you wish to attend, please send an e-mail to the organizer Serhii Yaskovets , so we can have a tally and book seminar rooms accordingly. Thanks! Registration deadline: March 1, 2024. Proposed topics Make OpenFPM available through Homebrew Extend template support in vector_dist_expressions to expressions with three-index operands Extend IO options in openfpm_io to support exporting tensor data types in vtp format Enable initial distribution of particles based on centroidal voronoi tessellation Integrate an interactive virtual reality in situ visualization framework with OpenFPM Rewrite DC-PSE implementation to reuse Verlet List from openfpm_data . Extend Verlet List to variable neighborhood size for individual particles Profile Performance of symmetrical particle interaction schemes Improve code documentation, code test coverage, guidance material availability *call for topics is still on","title":"Hackathon 2023/2024"},{"location":"hackathon24/#openfpm-hackathon-spring-2024","text":"","title":"OpenFPM Hackathon: Spring 2024"},{"location":"hackathon24/#openfpm-hackathon","text":"We are pleased to announce OpenFPM Hackathon 2024 to take place March 4 - March 8, 2024 in Munich. With OpenFPM developing into a global open-source project for scalable scientific computing, the community desires to meet in order to synchronize contributions, review and produce code, and get to know the core developers personally. This year we have 5-day in-person Hackathon, which is aimed at users, contributors, and developers. The core OpenFPM Developers will be present and available during the entire hackathon, so this is a great opportunity to get your ideas and wishes into the project and start close collaborations with them. Additionally, we welcome potential new users and people who are interested in learning more about OpenFPM in order to decide whether to start using it. You will have all the expert users and developers on site to talk to and answer your questions, and you can implement a first own hands-on example implementation to get familiar with OpenFPM and its usage. Last year's event made multiple algorithmic and computational ideas come true. Be part of it in 2024!","title":"OpenFPM Hackathon"},{"location":"hackathon24/#location","text":"Munich Institute of Integrated Materials, Energy and Process Engineering Seminar-Room Ground floor Technische Universit\u00e4t M\u00fcnchen Lichtenbergstr. 4a 85748 Garching","title":"Location"},{"location":"hackathon24/#timeplan-details-subject-to-change","text":"Day 1 - March 4: 09.00 - 12.30 Introduction to Openfpm: architecture, paradigms of development, application use-cases 12.30 - 13.30 Lunch break 13.30 - 15.00 Discussion: future of OpenFPM - its target audience, missing modules/features, ways to improve 15.00 - 16.00 Open problems: code review 16.00 - 18.00 Discussion: current issues and ways to solve them Day 2 - March 5: 09.00 - 12.30 Open problems: coding session 1 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: coding session 2 16.30 - 18.00 Discussion: sustainability of OpenFPM in the realm of scientific software Day 3 - March 6: 09.00 - 12.30 Open problems: coding session 1 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: coding session 2 16.30 - 18.00 Discussion: the art of reproducibility - best practices Day 4 - March 7: 09.00 - 12.30 Open problems: coding session 1 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: coding session 2 16.30 - 18.00 Discussion: integration with GitHub Day 5 - March 8 09.00 - 12.30 Open problems: wrap-up session 12.30 - 13.30 Lunch break 13.30 - 16.30 Open problems: plenary presentation & discussion","title":"Timeplan (details subject to change):"},{"location":"hackathon24/#travel-logistics-and-fee","text":"Participation in the hackathon is free of charge. There is no registration fee. Participants are expected to book and cover their travel arrangements by themselves. We can not provide financial or administrative support for travel to and from Munich. Also, food during the hackathon is on self-pay basis. Please find more information about Munich public transport network here . We can recommend the following hotels in the vicinity: Stellaris Apartment Hotel , on-campus. Booking codes for discounts at this place are available upon registration (see below) Hotel Marriott , on-campus. Booking codes for discounts at this place are available upon registration (see below) Hotel Koenig Ludwig II , Garching (off-campus), 5 min. Public transport to the campus Please do not hesitate to get in touch with us if you have questions or require assistance with hotel booking.","title":"Travel logistics and fee:"},{"location":"hackathon24/#registration","text":"Everyone interested is welcome to attend, regardless if you are already an OpenFPM crack, a beginner, or not yet using it at all; regardless if you want to contribute code, learn for yourself, or work on documentation and web page. There is something for every skillset. If you wish to attend, please send an e-mail to the organizer Serhii Yaskovets , so we can have a tally and book seminar rooms accordingly. Thanks! Registration deadline: March 1, 2024.","title":"Registration:"},{"location":"hackathon24/#proposed-topics","text":"Make OpenFPM available through Homebrew Extend template support in vector_dist_expressions to expressions with three-index operands Extend IO options in openfpm_io to support exporting tensor data types in vtp format Enable initial distribution of particles based on centroidal voronoi tessellation Integrate an interactive virtual reality in situ visualization framework with OpenFPM Rewrite DC-PSE implementation to reuse Verlet List from openfpm_data . Extend Verlet List to variable neighborhood size for individual particles Profile Performance of symmetrical particle interaction schemes Improve code documentation, code test coverage, guidance material availability *call for topics is still on","title":"Proposed topics"},{"location":"news/","text":"OpenFPM Versions 12/2/2024 OpenFPM 5.0.0 29/3/2022 OpenFPM 4.1.0 1/9/2021 OpenFPM 4.0.0 24/4/2021 OpenFPM 3.3.0 8/4/2021 OpenFPM 3.2.0 19/10/2020 OpenFPM 3.1.0 15/7/2020 OpenFPM 3.0.0 19/2/2019 OpenFPM 2.0.0 20/4/2018 OpenFPM 1.1.0 27/3/2018 OpenFPM 1.0.1 13/9/2017 OpenFPM 1.0.0 28/2/2017 OpenFPM 0.8.0 27/01/2017 OpenFPM 0.7.1 15/12/2016 OpenFPM 0.7.0 5/11/2016 OpenFPM 0.6.0 Change Log OpenFPM 4.1.0 - Mar 2022 On a general base the code should not use CUDA_ON_CPU but if it does CUDA_ON_CPU macro now cover both SEQUENTIAL and OpenMP backend. The macros CUDIFY_USE_CUDA,CUDIFY_USE_HIP,CUDIFY_USE_OPENMP,CUDIFY_USE_SEQUENTIAL,CUDIFY_USE_NONE can be checked to control which CUDA backend is used Fixed Minors bug OpenFPM 4.0.0 - Sep 2021 Adding DCPSE , Level-set based numerics (Closest-point) Changes Particles writer use now the new XML pvtp Paraview format Particles constructor does not accept discordance precision on space example: If I create a Particle set with vector_dist<3,double,.......> I MUST give in the constructor a domain Box<3,double> and a Ghost<3,double> , giving a Ghost<3,float> will produce an error OpenFPM 3.3.0 - Apr 2021 Adding support for HIP and AMD GPU. (Only particles) SPH example are compatible with HIP Additional Notes: WARNING: AMD GPUs are tested manually and not in CI. This mean that out this release stuff can break at least until I do not convince my working place to buy one for me ... and is gonna be hard because or rule here and there ... or who is reading this message does not want to buy one for me :-) SparseGridGPU are unsupported untill AMD does not fix the bug reported here : Changes None Fixed uninitialized variables in the SPH example on GPU, and other fixes necessary for AMD gpus OpenFPM 3.2.0 - Jan 2021 Adding CUDA_ON_CPU option to run CUDA code on CPU Adding gdb-gui debugger Fixed Minors bugs Changes In order to compile OpenFPM is now required a compiler implementing C++14 Standard OpenFPM 3.1.0 - Oct 2020 Adding GPU support for ghost_put Adding support for CUDA 11 Fixed Few details in the installation system of linear algebra for OSX Changed None OpenFPM 3.0.0 - Jul 2020 Upgrading all the dependencies: BOOST,PETSC,SUITESPARSE,OPENBLAS Adding CPU and GPU sparse grids. Look at the examples SparseGrid in the forlder examples Improving performance of GPU function remove_marked Fixed Several installation bugs on PETSC installation Changed Name for GoogleCharts has been changed please look at the examples OpenFPM 1.X - End of life (Theese versions are not supported enymore ) OpenFPM 2.0.0 - Feb 2019 Added Adding GPU support (see example 1_gpu_first_step 3_molecular_dynamic_gpu 7_sph_dlb_gpu 7_sph_dlb_gpu_opt) Fixed Detection of clang 10.0.0 on mac-osx mojave In VTK binary format all 64 bit types are casted to 32 bit. Either the long/unsigned_long are bugged in Paraview we tested, either I do not understand how they work. Changed The type Vcluster now is templated and the standard Vcluster is Vcluster<> Most probably you have to change in your code from Vcluster to Vcluster<> OpenFPM 1.1.1 - Dec 2018 Fixed Detection of clang 10.0.0 on mac-osx mojave OpenFPM 1.0.X - End of life (Theese versions are not supported enymore) OpenFPM 1.1.0 - Feb 2018 Added Interface for Multi-vector dynamic load balancing Increaded performance for grid ghost get Introduced forms to increase the performance of the grid iterator in case of stencil code (see example 5_GrayScott) EMatrix wrapped eigen matrices compatibles with vector_dist_id General tuning for high dimension vector_dist_id (up to 50 dimensions) + PS_CMA_ES (Particle-Swarm Covariant Matrix Adaptation Evolution Strategy) example in Numerics Added Discrete element Method example (8_DEM) Added serial_to_parallel example VCluster (2_serial_to_parallel). The example it show how to port a serial example into openfpm gradually swtiching from a serial section to a parallel section Introduced map(LOCAL) for fast communication in case we have small movement Fixed Installation/detection of PETSC CRITICAL-BUG scalar product in combination with vector product is broken (it return 0) Fixing 2D IO in binary for vector Fixing 1D grid writer in ASCII mode Fixing Intel compilation of Linear algebra OpenFPM 1.0.0 - Sep 2017 Added Introduced getDomainIterator for Cell-list New dynamic load balancing scheme to 7_SPH_opt (see Vector/7_SPH_opt) Increased performance of 7_SPH_opt Vortex in Cell example Numerics/Vortex_in_cell Interpolation functions (see Numerics/vortex_in_cell example) Gray-scott 3D example with stencil iterator optimization (see Grid/gray_scott_3d example) HDF5 Check point restart for vector_dist particles (see Vector/1_HDF5_save_and_load) Raw reader for grid (see ...) Added setPropNames to give names to properties (Grid and Vector see Vector/0_simple) Ghost put on grid (see Numerics/Vortex_in_Cell example) Stencil iterators for faster stencil codes see (Test3D_stencil function in Units tests src/Grid/Iterators/grid_dist_id_iterators_unit_tests.hpp) Algebraic multigrid solvers interface for linear systems (see Vortex in Cell example) Added setPropNames in vector_dist see Vector/0_simple Support for Windows with CYGWIN Fixed Bug fixes in installation of PETSC 2 Bugs in 7_SPH_opt and 7_SPH_opt error in Kernel and update for boundary particles Bug in VTK writer binary in case of vectors Bug in VTK writer binary: long int are not supported removing output Bug in FDScheme in the constructor with stencil bigger than one Bug Fixed Memory leak in petsc solver Bug Performance bug in the grid iterator Changed CAREFULL: write(\"output\",frame) now has changed to write_frame(\"output\",frame) write() with two arguments has a different meanings write(\"output\",options) getCellList and getCellListSym now return respectively CellList_gen > CellList > getIterator in CellList changed getCellIterator Grid iterator types has changes (one additional template parameter) FDScheme the constructor now has one parameter less (Parameter number 4 has been removed) (see Stokes_Flow examples in Numerics) MPI,PETSC,SUPERLU,Trilinos,MUMPS,SuiteSparse has been upgraded OpenFPM 0.8.0 - Feb 2017 Added Dynamic Load balancing Added SPH Dam break with Dynamic load balancing (7_sph_dlb)(7_sph_dlb_opt) Added automatic procedure for update ./install --update and --upgrade (From 0.8.0 version will have a long term support for bug fixing. End-of-life of 0.8.0 is not decided yet, it should be still supported for bug fixing after release 0.9.0) Added video lessons for Dynamic load balancing (openfpm.mpi-cbg.de) (website officially open) Added for debugging the options PRINT_STACKTRACE, CHECKFOR_POSNAN, CHECKFOR_POSINF, CHECKFOR_PROPINF, CHECKFOR_PROPNAN, SE_CLASS3. Added the possibility to write binary VTK files using VTK_WRITER and FORMAT_BINARY see 0_simple_vector for an example Fixed Installation of PETSC with MUMPS Changed BOOST updated to 1.63 Eigen updated to 3.3.7 OpenFPM 0.7.1 - Jan 2017 Fixed Multiphase verlet single to all case generate overflow OpenFPM 0.7.0 - Dec 2016 Added Symmetric cell-list/verlet list Crossing scheme VCluster examples Cell-list crossing scheme Fixed CRITICAL BUG: OpenFPM has a bug handling decomposition when a processor has a disconnected domains (By experience this case has been seen on big number of processors). Found and fixed a memory leak when using complex properties Changed The file VCluster has been mooved #include \"VCluster.hpp\" must be changed to #include \"VCluster/VCluster.hpp\" BECAUSE OF THIS, PLEASE CLEAN THE OPENFPM FOLDER OTHERWISE YOU WILL END TO HAVE 2 VCLUSTER.HPP OpenFPM 0.6.0 - Nov 2016 Added Symmetric cell-list/verlet list Multi-phase cell-list and Multi-phase cell-list Added ghost_get that keep properties Examples: 1_ghost_get_put it show how to use ghost_get and put with the new options 4_multiphase_celllist_verlet completely rewritten for new Cell-list and multiphase verlet 5_molecular_dynamic use case of symmetric cell-list and verlet list with ghost put 6_complex_usage It show how the flexibility of openfpm can be used to debug your program Plotting system can export graph in svg (to be included in the paper) Fixed Option NO_POSITION was untested Regression: Examples code compilation was broken on OSX (Affect only 0.5.1) (Internal: Added OSX examples compilarion/running test in the release pipeline) gray_scott example code (variable not initialized) Changes OpenFPM 0.5.1 - Sep 2016 Added ghost_put support for particles Full-Support for complex property on vector_dist (Serialization) Added examples for serialization of complex properties 4_Vector improved speed of the iterators Fixed Installation PETSC installation fail in case of preinstalled MPI Miss-compilation of SUITESPARSE on gcc-6.2 vector_dist with negative domain (Now supported) Grid 1D has been fixed One constructor of Box had arguments inverted. PLEASE CAREFULL ON THIS BUG float xmin[] = {0.0,0.0}; float xmax[] = {1.0,1.0}; // Box<2,float> box(xmax,xmin) BUG IT WAS xmax,xmin Box<2,float> box(xmin,xmax) <--- NOW IT IS xmin,xmax Box<2,float> box({0.0,0.0},{1.0,1.0}) <---- This constructor is not affected by the BUG Changed On gcc the -fext-numeric-literals compilation flag is now mandatory OpenFPM 0.5.0 - Aug 2016 Added map communicate particles across processors mooving the information of all the particle map_list give the possibility to give a list of property to move from one to another processor Numeric: Finite Differences discretization with matrix contruction and parallel solvers (See example ... ) vector_dist now support complex object like Point VectorS Box ... , with no limitation and more generic object like std::vector ... (WARNING TEMPORARY LIMITATION: Communication is not supported property must be excluded from communication using map_list and ghost_get) vector_dist support expressions (See example ...) No limit to ghost extension (they can be arbitrary extended) Multi-phase CellList Hilber curve data and computation reordering for cache firndliness Fixed Removed small crash for small grid and big number of processors Changed Known Bugs On gcc 6.1 the project does not compile Distributed grids on 1D do not work OpenFPM 0.4.0 - May 2016 Added Grid with periodic boundary conditions VTK Writer for distributed vector, now is the default writer Installation of linear algebra packages More user friendly installation (No environment variables to add in your bashrc, installation report less verbose) Fixed GPU compilation PARMetis automated installation Critical Bug in getCellList, it was producing Celllist with smaller spacing Changed OpenFPM 0.3.0 - Apr 2016 Added Molacular Dynamic example addUpdateCell list for more optimal update of the cell list instead of recreate the CellList Fixed Nothing to report Changed Eliminated global_v_cluster, init_global_v_cluster, delete_global_v_cluster, substituted by create_vcluster, openfpm_init, openfpm_finalize CartDecomposition parameter for the distributed structures is now optional template getPos<0>(), substituted by getPos() OpenFPM 0.2.1 - Apr 2016 Changed GoogleChart name function changed: AddPointGraph to AddLinesGraph and AddColumsGraph to AddHistGraph OpenFPM 0.2.0 - Mar 2016 Added Added Load Balancing and Dynamic Load Balancing on Beta PSE 1D example with multiple precision Plot example for GoogleChart plotting Distributed data structure now support 128bit floating point precision (on Beta) Fixed Detection 32 bit system and report as an error Bug in rounding off for periodic boundary condition Changed Nothing to report OpenFPM 0.1.0 - Feb 2016 Added PSE 1D example Cell list example Verlet list example Kickstart for OpenFPM_numeric Automated dependency installation for SUITESPRASE EIGEN OPENBLAS(LAPACK) Fixed CRITICAL BUG in periodic bondary condition BOOST auto updated to 1.60 Compilation with multiple .cpp files Changed Nothing to report","title":"News"},{"location":"news/#openfpm-versions","text":"12/2/2024 OpenFPM 5.0.0 29/3/2022 OpenFPM 4.1.0 1/9/2021 OpenFPM 4.0.0 24/4/2021 OpenFPM 3.3.0 8/4/2021 OpenFPM 3.2.0 19/10/2020 OpenFPM 3.1.0 15/7/2020 OpenFPM 3.0.0 19/2/2019 OpenFPM 2.0.0 20/4/2018 OpenFPM 1.1.0 27/3/2018 OpenFPM 1.0.1 13/9/2017 OpenFPM 1.0.0 28/2/2017 OpenFPM 0.8.0 27/01/2017 OpenFPM 0.7.1 15/12/2016 OpenFPM 0.7.0 5/11/2016 OpenFPM 0.6.0","title":"OpenFPM Versions"},{"location":"news/#change-log","text":"","title":"Change Log"},{"location":"news/#openfpm-410-mar-2022","text":"On a general base the code should not use CUDA_ON_CPU but if it does CUDA_ON_CPU macro now cover both SEQUENTIAL and OpenMP backend. The macros CUDIFY_USE_CUDA,CUDIFY_USE_HIP,CUDIFY_USE_OPENMP,CUDIFY_USE_SEQUENTIAL,CUDIFY_USE_NONE can be checked to control which CUDA backend is used","title":"OpenFPM 4.1.0 - Mar 2022"},{"location":"news/#fixed","text":"Minors bug","title":"Fixed"},{"location":"news/#openfpm-400-sep-2021","text":"Adding DCPSE , Level-set based numerics (Closest-point)","title":"OpenFPM 4.0.0 - Sep 2021"},{"location":"news/#changes","text":"Particles writer use now the new XML pvtp Paraview format Particles constructor does not accept discordance precision on space example: If I create a Particle set with vector_dist<3,double,.......> I MUST give in the constructor a domain Box<3,double> and a Ghost<3,double> , giving a Ghost<3,float> will produce an error","title":"Changes"},{"location":"news/#openfpm-330-apr-2021","text":"Adding support for HIP and AMD GPU. (Only particles) SPH example are compatible with HIP Additional Notes: WARNING: AMD GPUs are tested manually and not in CI. This mean that out this release stuff can break at least until I do not convince my working place to buy one for me ... and is gonna be hard because or rule here and there ... or who is reading this message does not want to buy one for me :-) SparseGridGPU are unsupported untill AMD does not fix the bug reported here :","title":"OpenFPM 3.3.0 - Apr 2021"},{"location":"news/#changes_1","text":"None","title":"Changes"},{"location":"news/#fixed_1","text":"uninitialized variables in the SPH example on GPU, and other fixes necessary for AMD gpus","title":"Fixed"},{"location":"news/#openfpm-320-jan-2021","text":"Adding CUDA_ON_CPU option to run CUDA code on CPU Adding gdb-gui debugger","title":"OpenFPM 3.2.0 - Jan 2021"},{"location":"news/#fixed_2","text":"Minors bugs","title":"Fixed"},{"location":"news/#changes_2","text":"In order to compile OpenFPM is now required a compiler implementing C++14 Standard","title":"Changes"},{"location":"news/#openfpm-310-oct-2020","text":"Adding GPU support for ghost_put Adding support for CUDA 11","title":"OpenFPM 3.1.0 - Oct 2020"},{"location":"news/#fixed_3","text":"Few details in the installation system of linear algebra for OSX","title":"Fixed"},{"location":"news/#changed","text":"None","title":"Changed"},{"location":"news/#openfpm-300-jul-2020","text":"Upgrading all the dependencies: BOOST,PETSC,SUITESPARSE,OPENBLAS Adding CPU and GPU sparse grids. Look at the examples SparseGrid in the forlder examples Improving performance of GPU function remove_marked","title":"OpenFPM 3.0.0 - Jul 2020"},{"location":"news/#fixed_4","text":"Several installation bugs on PETSC installation","title":"Fixed"},{"location":"news/#changed_1","text":"Name for GoogleCharts has been changed please look at the examples","title":"Changed"},{"location":"news/#openfpm-1x-end-of-life-theese-versions-are-not-supported-enymore","text":"","title":"OpenFPM 1.X - End of life (Theese versions are not supported enymore )"},{"location":"news/#openfpm-200-feb-2019","text":"","title":"OpenFPM 2.0.0 - Feb 2019"},{"location":"news/#added","text":"Adding GPU support (see example 1_gpu_first_step 3_molecular_dynamic_gpu 7_sph_dlb_gpu 7_sph_dlb_gpu_opt)","title":"Added"},{"location":"news/#fixed_5","text":"Detection of clang 10.0.0 on mac-osx mojave In VTK binary format all 64 bit types are casted to 32 bit. Either the long/unsigned_long are bugged in Paraview we tested, either I do not understand how they work.","title":"Fixed"},{"location":"news/#changed_2","text":"The type Vcluster now is templated and the standard Vcluster is Vcluster<> Most probably you have to change in your code from Vcluster to Vcluster<>","title":"Changed"},{"location":"news/#openfpm-111-dec-2018","text":"","title":"OpenFPM 1.1.1 - Dec 2018"},{"location":"news/#fixed_6","text":"Detection of clang 10.0.0 on mac-osx mojave","title":"Fixed"},{"location":"news/#openfpm-10x-end-of-life-theese-versions-are-not-supported-enymore","text":"","title":"OpenFPM 1.0.X - End of life (Theese versions are not supported enymore)"},{"location":"news/#openfpm-110-feb-2018","text":"","title":"OpenFPM 1.1.0 - Feb 2018"},{"location":"news/#added_1","text":"Interface for Multi-vector dynamic load balancing Increaded performance for grid ghost get Introduced forms to increase the performance of the grid iterator in case of stencil code (see example 5_GrayScott) EMatrix wrapped eigen matrices compatibles with vector_dist_id General tuning for high dimension vector_dist_id (up to 50 dimensions) + PS_CMA_ES (Particle-Swarm Covariant Matrix Adaptation Evolution Strategy) example in Numerics Added Discrete element Method example (8_DEM) Added serial_to_parallel example VCluster (2_serial_to_parallel). The example it show how to port a serial example into openfpm gradually swtiching from a serial section to a parallel section Introduced map(LOCAL) for fast communication in case we have small movement","title":"Added"},{"location":"news/#fixed_7","text":"Installation/detection of PETSC CRITICAL-BUG scalar product in combination with vector product is broken (it return 0) Fixing 2D IO in binary for vector Fixing 1D grid writer in ASCII mode Fixing Intel compilation of Linear algebra","title":"Fixed"},{"location":"news/#openfpm-100-sep-2017","text":"","title":"OpenFPM 1.0.0 - Sep 2017"},{"location":"news/#added_2","text":"Introduced getDomainIterator for Cell-list New dynamic load balancing scheme to 7_SPH_opt (see Vector/7_SPH_opt) Increased performance of 7_SPH_opt Vortex in Cell example Numerics/Vortex_in_cell Interpolation functions (see Numerics/vortex_in_cell example) Gray-scott 3D example with stencil iterator optimization (see Grid/gray_scott_3d example) HDF5 Check point restart for vector_dist particles (see Vector/1_HDF5_save_and_load) Raw reader for grid (see ...) Added setPropNames to give names to properties (Grid and Vector see Vector/0_simple) Ghost put on grid (see Numerics/Vortex_in_Cell example) Stencil iterators for faster stencil codes see (Test3D_stencil function in Units tests src/Grid/Iterators/grid_dist_id_iterators_unit_tests.hpp) Algebraic multigrid solvers interface for linear systems (see Vortex in Cell example) Added setPropNames in vector_dist see Vector/0_simple Support for Windows with CYGWIN","title":"Added"},{"location":"news/#fixed_8","text":"Bug fixes in installation of PETSC 2 Bugs in 7_SPH_opt and 7_SPH_opt error in Kernel and update for boundary particles Bug in VTK writer binary in case of vectors Bug in VTK writer binary: long int are not supported removing output Bug in FDScheme in the constructor with stencil bigger than one Bug Fixed Memory leak in petsc solver Bug Performance bug in the grid iterator","title":"Fixed"},{"location":"news/#changed_3","text":"CAREFULL: write(\"output\",frame) now has changed to write_frame(\"output\",frame) write() with two arguments has a different meanings write(\"output\",options) getCellList and getCellListSym now return respectively CellList_gen > CellList > getIterator in CellList changed getCellIterator Grid iterator types has changes (one additional template parameter) FDScheme the constructor now has one parameter less (Parameter number 4 has been removed) (see Stokes_Flow examples in Numerics) MPI,PETSC,SUPERLU,Trilinos,MUMPS,SuiteSparse has been upgraded","title":"Changed"},{"location":"news/#openfpm-080-feb-2017","text":"","title":"OpenFPM 0.8.0 - Feb 2017"},{"location":"news/#added_3","text":"Dynamic Load balancing Added SPH Dam break with Dynamic load balancing (7_sph_dlb)(7_sph_dlb_opt) Added automatic procedure for update ./install --update and --upgrade (From 0.8.0 version will have a long term support for bug fixing. End-of-life of 0.8.0 is not decided yet, it should be still supported for bug fixing after release 0.9.0) Added video lessons for Dynamic load balancing (openfpm.mpi-cbg.de) (website officially open) Added for debugging the options PRINT_STACKTRACE, CHECKFOR_POSNAN, CHECKFOR_POSINF, CHECKFOR_PROPINF, CHECKFOR_PROPNAN, SE_CLASS3. Added the possibility to write binary VTK files using VTK_WRITER and FORMAT_BINARY see 0_simple_vector for an example","title":"Added"},{"location":"news/#fixed_9","text":"Installation of PETSC with MUMPS","title":"Fixed"},{"location":"news/#changed_4","text":"BOOST updated to 1.63 Eigen updated to 3.3.7","title":"Changed"},{"location":"news/#openfpm-071-jan-2017","text":"","title":"OpenFPM 0.7.1 - Jan 2017"},{"location":"news/#fixed_10","text":"Multiphase verlet single to all case generate overflow","title":"Fixed"},{"location":"news/#openfpm-070-dec-2016","text":"","title":"OpenFPM 0.7.0 - Dec 2016"},{"location":"news/#added_4","text":"Symmetric cell-list/verlet list Crossing scheme VCluster examples Cell-list crossing scheme","title":"Added"},{"location":"news/#fixed_11","text":"CRITICAL BUG: OpenFPM has a bug handling decomposition when a processor has a disconnected domains (By experience this case has been seen on big number of processors). Found and fixed a memory leak when using complex properties","title":"Fixed"},{"location":"news/#changed_5","text":"The file VCluster has been mooved #include \"VCluster.hpp\" must be changed to #include \"VCluster/VCluster.hpp\" BECAUSE OF THIS, PLEASE CLEAN THE OPENFPM FOLDER OTHERWISE YOU WILL END TO HAVE 2 VCLUSTER.HPP","title":"Changed"},{"location":"news/#openfpm-060-nov-2016","text":"","title":"OpenFPM 0.6.0 - Nov 2016"},{"location":"news/#added_5","text":"Symmetric cell-list/verlet list Multi-phase cell-list and Multi-phase cell-list Added ghost_get that keep properties Examples: 1_ghost_get_put it show how to use ghost_get and put with the new options 4_multiphase_celllist_verlet completely rewritten for new Cell-list and multiphase verlet 5_molecular_dynamic use case of symmetric cell-list and verlet list with ghost put 6_complex_usage It show how the flexibility of openfpm can be used to debug your program Plotting system can export graph in svg (to be included in the paper)","title":"Added"},{"location":"news/#fixed_12","text":"Option NO_POSITION was untested Regression: Examples code compilation was broken on OSX (Affect only 0.5.1) (Internal: Added OSX examples compilarion/running test in the release pipeline) gray_scott example code (variable not initialized)","title":"Fixed"},{"location":"news/#changes_3","text":"","title":"Changes"},{"location":"news/#openfpm-051-sep-2016","text":"","title":"OpenFPM 0.5.1 - Sep 2016"},{"location":"news/#added_6","text":"ghost_put support for particles Full-Support for complex property on vector_dist (Serialization) Added examples for serialization of complex properties 4_Vector improved speed of the iterators","title":"Added"},{"location":"news/#fixed_13","text":"Installation PETSC installation fail in case of preinstalled MPI Miss-compilation of SUITESPARSE on gcc-6.2 vector_dist with negative domain (Now supported) Grid 1D has been fixed One constructor of Box had arguments inverted. PLEASE CAREFULL ON THIS BUG float xmin[] = {0.0,0.0}; float xmax[] = {1.0,1.0}; // Box<2,float> box(xmax,xmin) BUG IT WAS xmax,xmin Box<2,float> box(xmin,xmax) <--- NOW IT IS xmin,xmax Box<2,float> box({0.0,0.0},{1.0,1.0}) <---- This constructor is not affected by the BUG","title":"Fixed"},{"location":"news/#changed_6","text":"On gcc the -fext-numeric-literals compilation flag is now mandatory","title":"Changed"},{"location":"news/#openfpm-050-aug-2016","text":"","title":"OpenFPM 0.5.0 - Aug 2016"},{"location":"news/#added_7","text":"map communicate particles across processors mooving the information of all the particle map_list give the possibility to give a list of property to move from one to another processor Numeric: Finite Differences discretization with matrix contruction and parallel solvers (See example ... ) vector_dist now support complex object like Point VectorS Box ... , with no limitation and more generic object like std::vector ... (WARNING TEMPORARY LIMITATION: Communication is not supported property must be excluded from communication using map_list and ghost_get) vector_dist support expressions (See example ...) No limit to ghost extension (they can be arbitrary extended) Multi-phase CellList Hilber curve data and computation reordering for cache firndliness","title":"Added"},{"location":"news/#fixed_14","text":"Removed small crash for small grid and big number of processors","title":"Fixed"},{"location":"news/#changed_7","text":"","title":"Changed"},{"location":"news/#known-bugs","text":"On gcc 6.1 the project does not compile Distributed grids on 1D do not work","title":"Known Bugs"},{"location":"news/#openfpm-040-may-2016","text":"","title":"OpenFPM 0.4.0 - May 2016"},{"location":"news/#added_8","text":"Grid with periodic boundary conditions VTK Writer for distributed vector, now is the default writer Installation of linear algebra packages More user friendly installation (No environment variables to add in your bashrc, installation report less verbose)","title":"Added"},{"location":"news/#fixed_15","text":"GPU compilation PARMetis automated installation Critical Bug in getCellList, it was producing Celllist with smaller spacing","title":"Fixed"},{"location":"news/#changed_8","text":"","title":"Changed"},{"location":"news/#openfpm-030-apr-2016","text":"","title":"OpenFPM 0.3.0 - Apr 2016"},{"location":"news/#added_9","text":"Molacular Dynamic example addUpdateCell list for more optimal update of the cell list instead of recreate the CellList","title":"Added"},{"location":"news/#fixed_16","text":"Nothing to report","title":"Fixed"},{"location":"news/#changed_9","text":"Eliminated global_v_cluster, init_global_v_cluster, delete_global_v_cluster, substituted by create_vcluster, openfpm_init, openfpm_finalize CartDecomposition parameter for the distributed structures is now optional template getPos<0>(), substituted by getPos()","title":"Changed"},{"location":"news/#openfpm-021-apr-2016","text":"","title":"OpenFPM 0.2.1 - Apr 2016"},{"location":"news/#changed_10","text":"GoogleChart name function changed: AddPointGraph to AddLinesGraph and AddColumsGraph to AddHistGraph","title":"Changed"},{"location":"news/#openfpm-020-mar-2016","text":"","title":"OpenFPM 0.2.0 - Mar 2016"},{"location":"news/#added_10","text":"Added Load Balancing and Dynamic Load Balancing on Beta PSE 1D example with multiple precision Plot example for GoogleChart plotting Distributed data structure now support 128bit floating point precision (on Beta)","title":"Added"},{"location":"news/#fixed_17","text":"Detection 32 bit system and report as an error Bug in rounding off for periodic boundary condition","title":"Fixed"},{"location":"news/#changed_11","text":"Nothing to report","title":"Changed"},{"location":"news/#openfpm-010-feb-2016","text":"","title":"OpenFPM 0.1.0 - Feb 2016"},{"location":"news/#added_11","text":"PSE 1D example Cell list example Verlet list example Kickstart for OpenFPM_numeric Automated dependency installation for SUITESPRASE EIGEN OPENBLAS(LAPACK)","title":"Added"},{"location":"news/#fixed_18","text":"CRITICAL BUG in periodic bondary condition BOOST auto updated to 1.60 Compilation with multiple .cpp files","title":"Fixed"},{"location":"news/#changed_12","text":"Nothing to report","title":"Changed"},{"location":"vector-example/","text":"MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}}); For detailed documentation of the OpenFPM sources, including the examples, see the online Doxygen documentation . Vector 0: Simple vector initialization This example show several basic functionalities of the distributed vector vector_dist . The distributed vector is a set of particles in an N-dimensional space. In this example it is shown how to: Initialize the library Create a Box that defines the domain An array that defines the boundary conditions A Ghost object that will define the extension of the ghost part in physical units The source code of the example Vector/0_simple/main.cpp . The full doxygen documentation Vector_0_simple . See also our video lectures dedicated to this topic Video 1 , Video 2 Example 1: Vector Ghost layer This example shows the properties of ghost_get and ghost_put - functions that synchronize the ghosts layer for a distributed vector vector_dist . In this example it is shown how to: Iterate vector_dist via getDomainIterator Redistribute the particles in vector_dist according to the underlying domain decomposition via map Synchronize the ghost layers in the standard way NO_POSITION , KEEP_PROPERTIES and SKIP_LABELLING options of the ghost_get function Propagate the data from ghost to non-ghost particles via ghost_put The source code of the example Vector/1_ghost_get_put/main.cpp . The full doxygen documentation Vector_1_ghost_get . Example 2: Cell-lists and Verlet-lists This example shows the properties of ghost_get and ghost_put - functions that synchronize the ghosts layer for a distributed vector vector_dist . Key points: How to utilize the grid iterator getGridIterator , to create a grid-like particle domain Two principal types of fast neighbor lists: cell-list getCellList and Verlet-list getVerlet for a distributed vector vector_dist CELL_MEMFAST , CELL_MEMBAL and CELL_MEMMW variations of the cell-list, with different memory requirements and computations costs Iterating through the neighboring particles via getNNIterator of cell-list and Verlet-list The source code of the example Vector/1_celllist/main.cpp . The full doxygen documentation Vector_1_celllist . Example 3: GPU vector This example shows how to create a vector data-structure with vector_dist_gpu to access a vector_dist -alike data structure from GPU accelerated computing code. Key points: How to convert the source code from using vector_dist to vector_dist_gpu and how it influences the memory layout of the data structure Oflloading particle position hostToDevicePos and particle property hostToDeviceProp data from CPU to GPU Lanuching a CUDA-like kernel with CUDA_LAUNCH and automatic subdivision of a computation loop into workgroups/threads via getDomainIteratorGPU or manually specifying the number of workgroups and the number of threads in a workgroup Passing the data-structures to a CUDA-like kernel code via toKernel How to use map with the option RUN_DEVICE to redistribute the particles directly on GPU, and ghost_get with RUN_DEVICE option to fill ghost particles directly on GPU How to detect and utilize RDMA on GPU to get the support of CUDA-aware MPI implementation to work directly with device pointers in communication subroutines The source code of the example Vector/1_gpu_first_step/main.cpp . The full doxygen documentation Vector_1_gpu_first_step . Example 4: HDF5 Save and load This example show how to save and load a vector to/from the parallel file format HDF5. Key points: How to save the position/property information of the particles vector_dist into an .hdf5 file via save How to load the position/property information of the particles vector_dist from an .hdf5 file via load The source code of the example Vector/1_HDF5_save_load/main.cpp . The full doxygen documentation Vector_1_HDF5 . Example 5: Vector expressions This example shows how to use vector expressions to apply mathematical operations and functions on particles. The example also shows to create a point-wise applicable function $$ A_q e^{\\frac{|x_p-x_q|^2}{\\sigma}} $$ where $A_q$ is the property $A$ of particle $q$, $x_p, x_q$ are positions of particles $p, q$ correspondingly. Key points: Setting an alias for particle properties via getV of particle_dist to be used within an expression Composing expressions with scalar particle properties Composing expressions with vector particle properties. The expressions are 1) applied point-wise; 2) used to create a component-wise multiplication via * ; 3) scalar product via pmul ; 4) compute a norm norm ; 5) perform square root operation sqrt Converting Point object into an expression getVExpr to be used with vector expressions Utilizing operator= and the function assign to assing singular or multiple particle properties per iteration through particles Constructing expressions with applyKernel_in and applyKernel_in_gen to create kernel functions called at particle locations for all the neighboring particles, e.g. as in SPH $$\\sum_{q = Neighborhood(p)} A_q D^{\\beta}ker(x_p,x_q) V_q $$ The source code of the example Vector/2_expressions/main.cpp . The full doxygen documentation Vector_2_expression . Example 6: Molecular Dynamics with Lennard-Jones potential (Cell-List) This example shows a simple Lennard-Jones molecular dynamics simulation in a stable regime. The particles interact with the interaction potential $$ V(x_p,x_q) = 4( (\\frac{\\sigma}{r})^{12} - (\\frac{\\sigma}{r})^6 ) $$ $A_q$ is the property $A$ of particle $q$, $x_p, x_q$ are positions of particles $p, q$ correspondingly, $\\sigma$ is a free parameter, $r$ is the distance between the particles. Key points: Reusing memory allocated with getCellList for the subsequent iterations via updateCellList Utilizing CELL_MEMBAL with getCellList to minimize memory footprint Performing 10000 time steps using symplectic Verlet integrator $$ \\vec{v}(t_{n}+1/2) = \\vec{v}_p(t_n) + \\frac{1}{2} \\delta t \\vec{a}(t_n) $$ $$ \\vec{x}(t_{n}+1) = \\vec{x}_p(t_n) + \\delta t \\vec{v}(t_n+1/2) $$ $$ \\vec{v}(t_{n+1}) = \\vec{v}_p(t_n+1/2) + \\frac{1}{2} \\delta t \\vec{a}(t_n+1) $$ Producing a time-total energy 2D plot with GoogleChart The source code of the example Vector/3_molecular_dynamic/main.cpp . The full doxygen documentation Vector_3_md_dyn . Example 7: Molecular Dynamics with Lennard-Jones potential (Verlet-List) The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) . Please refer to it for futher details. Key points: Due to the computational cost of updating Verlet-list, r_cut + skin cutoff distance is used such that the Verlet-list has to be updated once in 10 iterations via updateVerlet As Verlet-lists are constructed based on local particle id's, which would be invalidated by map or ghost_get , map is called every 10 time-step, and ghost_get is used with SKIP_LABELLING option to keep old indices every iteration The source code of the example Vector/3_molecular_dynamic/main_vl.cpp . The full doxygen documentation Vector_3_md_vl . Example 15: Molecular Dynamics with Lennard-Jones potential (Symmetric Verlet-List) This example is an extension to Molecular Dynamics with Lennard-Jones potential (Verlet-List) . It shows how better performance can be achieved for symmetric interaction models with symmetric Verlet-list compared to the standard Verlet-list. Key points: Computing the interaction for particles p , q only once Propagate the data from potentially ghost particles q to non-ghost particles in their corresponding domains via ghost_put with the operation add_ Changing the prefactor in the subroutine of calculating the total energy as every pair of particles is visited once (as compared to two times before) Updating Verlet-list once in 10 iterations via updateVerlet with 'VL_SYMMETRIC' flag The source code of the example Vector/5_molecular_dynamic_sym/main.cpp . The full doxygen documentation Vector_5_md_vl_sym . Example 16: Molecular Dynamics with Lennard-Jones potential (Symmetric CRS Verlet-List) This example is an extension to Molecular Dynamics with Lennard-Jones potential (Verlet-List) and Molecular Dynamics with Lennard-Jones potential (Verlet-List) . It shows how better performance can be achieved for symmetric interaction models with symmetric Verlet-list compared to the standard Verlet-list. Key points: Computing the interaction for particles p , q only once Propagate the data from potentially ghost particles q to non-ghost particles in their corresponding domains via ghost_put with the operation add_ Changing the prefactor in the subroutine of calculating the total energy as every pair of particles is visited once (as compared to two times before) Updating Verlet-list once in 10 iterations via updateVerlet with 'VL_SYMMETRIC' flag The source code of the example Vector/5_molecular_dynamic_sym/main.cpp . The full doxygen documentation Vector_5_md_vl_sym . Example 8: Molecular Dynamics with Lennard-Jones potential (GPU) The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) and Molecular Dynamics with Lennard-Jones potential (Verlet-List) . Please refer to those for futher details. Key points: To get the particle index inside a CUDA-like kernel GET_PARTICLE macro is used to avoid overflow in the construction blockIdx.x * blockDim.x + threadIdx.x A primitive reduction function reduce_local with the operation _add_ is used to get the total energy by summing energies of all particles. The source code of the example Vector/3_molecular_dynamic_gpu/main_vl.cpp . The full doxygen documentation Vector_3_md_dyn_gpu . Example 9: Molecular Dynamics with Lennard-Jones potential (GPU optimized) The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) , Molecular Dynamics with Lennard-Jones potential (Verlet-List) and is based on Molecular Dynamics with Lennard-Jones potential (GPU) . Please refer to those for futher details. Key points: To achieve coalesced memory access on GPU and to reduce cache load the particle indices are stored in cell-list in a sorted manner, i.e. particles with neighboring indices are located in the same cell-list. This is achieved by assigning new particle indices and storing them temporarily in vector_dist . The sorted version of vector_dist_gpu is offloaded to GPU using toKernel_sorted . It uses get_sort instead of get to get a particle index in the cell-list neighborhood iterator getNNIteratorBox The sorted version of particle properties have to be merged to the original ones once the processing is done via merge_sort of vector_dist The source code of the example Vector/3_molecular_dynamic_gpu_opt/main_vl.cpp . The full doxygen documentation Vector_3_md_dyn_gpu_opt . Example 10: Molecular Dynamics with Lennard-Jones potential (Particle reordering) The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) , Molecular Dynamics with Lennard-Jones potential (Verlet-List) . The example shows how reordering the data can significantly reduce the computational running time. Key points: The particles inside vector_dist are reordered via reorder following a Hilbert curve of order m (here m=5 ) passing through the cells of $2^m \\times 2^m \\times 2^m$ (here, in 3D) cell-list It is shown that the frequency of reordering depends on the mobility of particles Wall clock time is measured of the function calc_force utilizing the object timer via start and stop The source code of the example Vector/4_reorder/main_data_ord.cpp . The full doxygen documentation Vector_4_reo . Example 11: Molecular Dynamics with Lennard-Jones potential (Cell-list reordering) The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) , Molecular Dynamics with Lennard-Jones potential (Verlet-List) . The example shows how reordering the data can significantly reduce the computational running time. Key points: The cell-list cells are iterated following a Hilbert curve instead of a normal left-to-right bottom-to-top cell iteration (in 2D). The function getCellList_hilb of vector_dist is used instead of getCellList It is shown that for static or slowly moving particles a speedup of up to 10% could be achieved The source code of the example Vector/4_reorder/main_comp_ord.cpp . The full doxygen documentation Vector_4_comp_reo . Example 12: Complex properties in Vector 1 This example shows how to use complex properties in the distributed vector vector_dist Key points: Creating a distributed vector with particle properties: scalar, vector float[3] , Point , list of float openfpm::vector<float> , list of custom structures openfpm::vector<A> (where A is a user-defined type with no pointers), vector of vectors openfpm::vector<openfpm::vector<float>>> Redistribute the particles in vector_dist according to the underlying domain decomposition. Communicate only the selected particle properties via map_list (instead of communicating all map ) Synchronize the ghost layers only for the selected particle properties ghost_get The source code of the example Vector/4_complex_prop/main.cpp . The full doxygen documentation Vector_4_complex_prop . Example 13: Complex properties in Vector 2 This example shows how to use complex properties in the distributed vector vector_dist Key points: Creating a distributed vector with particle properties: scalar, vector float[3] , Point , list of float openfpm::vector<float> , list of custom structures openfpm::vector<A> (where A is a user-defined type with memory pointers inside), vector of vectors openfpm::vector<openfpm::vector<float>>> Enabling the user-defined type being serializable by vector_dist via packRequest method to indicate how many byte are needed to serialize the structure pack method to serialize the data-structure via methods allocate , getPointer of ExtPreAlloc and method pack of Packer unpack method to deserialize the data-structure via method getPointerOffset of ExtPreAlloc and method unpack of Unpacker noPointers method to inform the serialization system that the object has pointers Constructing constructor, destructor and operator= to avoid memory leaks The source code of the example Vector/4_complex_prop/main.cpp . The full doxygen documentation Vector_4_complex_prop_ser . Example 14: Multiphase Cell-lists and Verlet-lists This example is an extension to Example 2: Cell-lists and Verlet-lists and ()[]. It shows how to use multi-phase cell-lists and Verlet-list using multiple instances of vector_dist . Key points: All the phases have to use the same domain decomposition, which is achieved by passing the decomposition of the first phase to the constructor of vector_dist of all the other phases. The domains have to be iterated individually via getDomainIterator , the particles redistributed via map , the ghost layers synchronized via ghost_get for all the phases vector_dist . Constructing Verlet-lists for two phases ( ph0 , ph1 ) with createVerlet , where for one phase ph0 the neighoring particles of ph1 are assigned in the Verlet-list. Cell-list of ph1 has to be passed to createVerlet Constructing Verlet-lists for multiple phases ( ph0 , ph1 , ph2 ...) with createVerletM , where for one phase ph0 the neighoring particles of ph1 , ph2 ... are assigned in the Verlet-list. Cell-list containing all of ph1 , ph2 ... create with createCellListM has to be passed to createVerletM Iterating over the neighboring particles of a multiphase Verlet-list with getNNIterator with get being substituded by getP (particle phase) and getV (particle id) Extending example of the symmetric interaction for multiphase cell-lists and Verlet-lists via createCellListSymM , createVerletSymM The source code of the example Vector/4_multiphase_celllist_verlet/main.cpp . The full doxygen documentation Vector_4_mp_cl . Example 16: Validation and debugging This example shows how the flexibility of the library can be used to perform complex tasks for validation and debugging. Key points: To get unique global id's of the particles the function accum of vector_dist is used, which returns prefix sum of local domain sizes $j<i$ for the logical processor $i$ out of $N$ total processors Propagate the data from potentially ghost particles q to non-ghost particles in their corresponding domains via ghost_put with the operation merge_ , that merges two openfpm::vector (ghost and non-ghost) The source code of the example Vector/6_complex_usage/main.cpp . The full doxygen documentation Vector_6_complex_usage .","title":"Vector"},{"location":"vector-example/#vector-0-simple-vector-initialization","text":"This example show several basic functionalities of the distributed vector vector_dist . The distributed vector is a set of particles in an N-dimensional space. In this example it is shown how to: Initialize the library Create a Box that defines the domain An array that defines the boundary conditions A Ghost object that will define the extension of the ghost part in physical units The source code of the example Vector/0_simple/main.cpp . The full doxygen documentation Vector_0_simple . See also our video lectures dedicated to this topic Video 1 , Video 2","title":"Vector 0: Simple vector initialization"},{"location":"vector-example/#example-1-vector-ghost-layer","text":"This example shows the properties of ghost_get and ghost_put - functions that synchronize the ghosts layer for a distributed vector vector_dist . In this example it is shown how to: Iterate vector_dist via getDomainIterator Redistribute the particles in vector_dist according to the underlying domain decomposition via map Synchronize the ghost layers in the standard way NO_POSITION , KEEP_PROPERTIES and SKIP_LABELLING options of the ghost_get function Propagate the data from ghost to non-ghost particles via ghost_put The source code of the example Vector/1_ghost_get_put/main.cpp . The full doxygen documentation Vector_1_ghost_get .","title":"Example 1: Vector Ghost layer"},{"location":"vector-example/#example-2-cell-lists-and-verlet-lists","text":"This example shows the properties of ghost_get and ghost_put - functions that synchronize the ghosts layer for a distributed vector vector_dist . Key points: How to utilize the grid iterator getGridIterator , to create a grid-like particle domain Two principal types of fast neighbor lists: cell-list getCellList and Verlet-list getVerlet for a distributed vector vector_dist CELL_MEMFAST , CELL_MEMBAL and CELL_MEMMW variations of the cell-list, with different memory requirements and computations costs Iterating through the neighboring particles via getNNIterator of cell-list and Verlet-list The source code of the example Vector/1_celllist/main.cpp . The full doxygen documentation Vector_1_celllist .","title":"Example 2: Cell-lists and Verlet-lists"},{"location":"vector-example/#example-3-gpu-vector","text":"This example shows how to create a vector data-structure with vector_dist_gpu to access a vector_dist -alike data structure from GPU accelerated computing code. Key points: How to convert the source code from using vector_dist to vector_dist_gpu and how it influences the memory layout of the data structure Oflloading particle position hostToDevicePos and particle property hostToDeviceProp data from CPU to GPU Lanuching a CUDA-like kernel with CUDA_LAUNCH and automatic subdivision of a computation loop into workgroups/threads via getDomainIteratorGPU or manually specifying the number of workgroups and the number of threads in a workgroup Passing the data-structures to a CUDA-like kernel code via toKernel How to use map with the option RUN_DEVICE to redistribute the particles directly on GPU, and ghost_get with RUN_DEVICE option to fill ghost particles directly on GPU How to detect and utilize RDMA on GPU to get the support of CUDA-aware MPI implementation to work directly with device pointers in communication subroutines The source code of the example Vector/1_gpu_first_step/main.cpp . The full doxygen documentation Vector_1_gpu_first_step .","title":"Example 3: GPU vector"},{"location":"vector-example/#example-4-hdf5-save-and-load","text":"This example show how to save and load a vector to/from the parallel file format HDF5. Key points: How to save the position/property information of the particles vector_dist into an .hdf5 file via save How to load the position/property information of the particles vector_dist from an .hdf5 file via load The source code of the example Vector/1_HDF5_save_load/main.cpp . The full doxygen documentation Vector_1_HDF5 .","title":"Example 4: HDF5 Save and load"},{"location":"vector-example/#example-5-vector-expressions","text":"This example shows how to use vector expressions to apply mathematical operations and functions on particles. The example also shows to create a point-wise applicable function $$ A_q e^{\\frac{|x_p-x_q|^2}{\\sigma}} $$ where $A_q$ is the property $A$ of particle $q$, $x_p, x_q$ are positions of particles $p, q$ correspondingly. Key points: Setting an alias for particle properties via getV of particle_dist to be used within an expression Composing expressions with scalar particle properties Composing expressions with vector particle properties. The expressions are 1) applied point-wise; 2) used to create a component-wise multiplication via * ; 3) scalar product via pmul ; 4) compute a norm norm ; 5) perform square root operation sqrt Converting Point object into an expression getVExpr to be used with vector expressions Utilizing operator= and the function assign to assing singular or multiple particle properties per iteration through particles Constructing expressions with applyKernel_in and applyKernel_in_gen to create kernel functions called at particle locations for all the neighboring particles, e.g. as in SPH $$\\sum_{q = Neighborhood(p)} A_q D^{\\beta}ker(x_p,x_q) V_q $$ The source code of the example Vector/2_expressions/main.cpp . The full doxygen documentation Vector_2_expression .","title":"Example 5: Vector expressions"},{"location":"vector-example/#example-6-molecular-dynamics-with-lennard-jones-potential-cell-list","text":"This example shows a simple Lennard-Jones molecular dynamics simulation in a stable regime. The particles interact with the interaction potential $$ V(x_p,x_q) = 4( (\\frac{\\sigma}{r})^{12} - (\\frac{\\sigma}{r})^6 ) $$ $A_q$ is the property $A$ of particle $q$, $x_p, x_q$ are positions of particles $p, q$ correspondingly, $\\sigma$ is a free parameter, $r$ is the distance between the particles. Key points: Reusing memory allocated with getCellList for the subsequent iterations via updateCellList Utilizing CELL_MEMBAL with getCellList to minimize memory footprint Performing 10000 time steps using symplectic Verlet integrator $$ \\vec{v}(t_{n}+1/2) = \\vec{v}_p(t_n) + \\frac{1}{2} \\delta t \\vec{a}(t_n) $$ $$ \\vec{x}(t_{n}+1) = \\vec{x}_p(t_n) + \\delta t \\vec{v}(t_n+1/2) $$ $$ \\vec{v}(t_{n+1}) = \\vec{v}_p(t_n+1/2) + \\frac{1}{2} \\delta t \\vec{a}(t_n+1) $$ Producing a time-total energy 2D plot with GoogleChart The source code of the example Vector/3_molecular_dynamic/main.cpp . The full doxygen documentation Vector_3_md_dyn .","title":"Example 6: Molecular Dynamics with Lennard-Jones potential (Cell-List)"},{"location":"vector-example/#example-7-molecular-dynamics-with-lennard-jones-potential-verlet-list","text":"The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) . Please refer to it for futher details. Key points: Due to the computational cost of updating Verlet-list, r_cut + skin cutoff distance is used such that the Verlet-list has to be updated once in 10 iterations via updateVerlet As Verlet-lists are constructed based on local particle id's, which would be invalidated by map or ghost_get , map is called every 10 time-step, and ghost_get is used with SKIP_LABELLING option to keep old indices every iteration The source code of the example Vector/3_molecular_dynamic/main_vl.cpp . The full doxygen documentation Vector_3_md_vl .","title":"Example 7: Molecular Dynamics with Lennard-Jones potential (Verlet-List)"},{"location":"vector-example/#example-15-molecular-dynamics-with-lennard-jones-potential-symmetric-verlet-list","text":"This example is an extension to Molecular Dynamics with Lennard-Jones potential (Verlet-List) . It shows how better performance can be achieved for symmetric interaction models with symmetric Verlet-list compared to the standard Verlet-list. Key points: Computing the interaction for particles p , q only once Propagate the data from potentially ghost particles q to non-ghost particles in their corresponding domains via ghost_put with the operation add_ Changing the prefactor in the subroutine of calculating the total energy as every pair of particles is visited once (as compared to two times before) Updating Verlet-list once in 10 iterations via updateVerlet with 'VL_SYMMETRIC' flag The source code of the example Vector/5_molecular_dynamic_sym/main.cpp . The full doxygen documentation Vector_5_md_vl_sym .","title":"Example 15: Molecular Dynamics with Lennard-Jones potential (Symmetric Verlet-List)"},{"location":"vector-example/#example-16-molecular-dynamics-with-lennard-jones-potential-symmetric-crs-verlet-list","text":"This example is an extension to Molecular Dynamics with Lennard-Jones potential (Verlet-List) and Molecular Dynamics with Lennard-Jones potential (Verlet-List) . It shows how better performance can be achieved for symmetric interaction models with symmetric Verlet-list compared to the standard Verlet-list. Key points: Computing the interaction for particles p , q only once Propagate the data from potentially ghost particles q to non-ghost particles in their corresponding domains via ghost_put with the operation add_ Changing the prefactor in the subroutine of calculating the total energy as every pair of particles is visited once (as compared to two times before) Updating Verlet-list once in 10 iterations via updateVerlet with 'VL_SYMMETRIC' flag The source code of the example Vector/5_molecular_dynamic_sym/main.cpp . The full doxygen documentation Vector_5_md_vl_sym .","title":"Example 16: Molecular Dynamics with Lennard-Jones potential (Symmetric CRS Verlet-List)"},{"location":"vector-example/#example-8-molecular-dynamics-with-lennard-jones-potential-gpu","text":"The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) and Molecular Dynamics with Lennard-Jones potential (Verlet-List) . Please refer to those for futher details. Key points: To get the particle index inside a CUDA-like kernel GET_PARTICLE macro is used to avoid overflow in the construction blockIdx.x * blockDim.x + threadIdx.x A primitive reduction function reduce_local with the operation _add_ is used to get the total energy by summing energies of all particles. The source code of the example Vector/3_molecular_dynamic_gpu/main_vl.cpp . The full doxygen documentation Vector_3_md_dyn_gpu .","title":"Example 8: Molecular Dynamics with Lennard-Jones potential (GPU)"},{"location":"vector-example/#example-9-molecular-dynamics-with-lennard-jones-potential-gpu-optimized","text":"The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) , Molecular Dynamics with Lennard-Jones potential (Verlet-List) and is based on Molecular Dynamics with Lennard-Jones potential (GPU) . Please refer to those for futher details. Key points: To achieve coalesced memory access on GPU and to reduce cache load the particle indices are stored in cell-list in a sorted manner, i.e. particles with neighboring indices are located in the same cell-list. This is achieved by assigning new particle indices and storing them temporarily in vector_dist . The sorted version of vector_dist_gpu is offloaded to GPU using toKernel_sorted . It uses get_sort instead of get to get a particle index in the cell-list neighborhood iterator getNNIteratorBox The sorted version of particle properties have to be merged to the original ones once the processing is done via merge_sort of vector_dist The source code of the example Vector/3_molecular_dynamic_gpu_opt/main_vl.cpp . The full doxygen documentation Vector_3_md_dyn_gpu_opt .","title":"Example 9: Molecular Dynamics with Lennard-Jones potential (GPU optimized)"},{"location":"vector-example/#example-10-molecular-dynamics-with-lennard-jones-potential-particle-reordering","text":"The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) , Molecular Dynamics with Lennard-Jones potential (Verlet-List) . The example shows how reordering the data can significantly reduce the computational running time. Key points: The particles inside vector_dist are reordered via reorder following a Hilbert curve of order m (here m=5 ) passing through the cells of $2^m \\times 2^m \\times 2^m$ (here, in 3D) cell-list It is shown that the frequency of reordering depends on the mobility of particles Wall clock time is measured of the function calc_force utilizing the object timer via start and stop The source code of the example Vector/4_reorder/main_data_ord.cpp . The full doxygen documentation Vector_4_reo .","title":"Example 10: Molecular Dynamics with Lennard-Jones potential (Particle reordering)"},{"location":"vector-example/#example-11-molecular-dynamics-with-lennard-jones-potential-cell-list-reordering","text":"The physical model in the example is identical to Molecular Dynamics with Lennard-Jones potential (Cell-List) , Molecular Dynamics with Lennard-Jones potential (Verlet-List) . The example shows how reordering the data can significantly reduce the computational running time. Key points: The cell-list cells are iterated following a Hilbert curve instead of a normal left-to-right bottom-to-top cell iteration (in 2D). The function getCellList_hilb of vector_dist is used instead of getCellList It is shown that for static or slowly moving particles a speedup of up to 10% could be achieved The source code of the example Vector/4_reorder/main_comp_ord.cpp . The full doxygen documentation Vector_4_comp_reo .","title":"Example 11: Molecular Dynamics with Lennard-Jones potential (Cell-list reordering)"},{"location":"vector-example/#example-12-complex-properties-in-vector-1","text":"This example shows how to use complex properties in the distributed vector vector_dist Key points: Creating a distributed vector with particle properties: scalar, vector float[3] , Point , list of float openfpm::vector<float> , list of custom structures openfpm::vector<A> (where A is a user-defined type with no pointers), vector of vectors openfpm::vector<openfpm::vector<float>>> Redistribute the particles in vector_dist according to the underlying domain decomposition. Communicate only the selected particle properties via map_list (instead of communicating all map ) Synchronize the ghost layers only for the selected particle properties ghost_get The source code of the example Vector/4_complex_prop/main.cpp . The full doxygen documentation Vector_4_complex_prop .","title":"Example 12: Complex properties in Vector 1"},{"location":"vector-example/#example-13-complex-properties-in-vector-2","text":"This example shows how to use complex properties in the distributed vector vector_dist Key points: Creating a distributed vector with particle properties: scalar, vector float[3] , Point , list of float openfpm::vector<float> , list of custom structures openfpm::vector<A> (where A is a user-defined type with memory pointers inside), vector of vectors openfpm::vector<openfpm::vector<float>>> Enabling the user-defined type being serializable by vector_dist via packRequest method to indicate how many byte are needed to serialize the structure pack method to serialize the data-structure via methods allocate , getPointer of ExtPreAlloc and method pack of Packer unpack method to deserialize the data-structure via method getPointerOffset of ExtPreAlloc and method unpack of Unpacker noPointers method to inform the serialization system that the object has pointers Constructing constructor, destructor and operator= to avoid memory leaks The source code of the example Vector/4_complex_prop/main.cpp . The full doxygen documentation Vector_4_complex_prop_ser .","title":"Example 13: Complex properties in Vector 2"},{"location":"vector-example/#example-14-multiphase-cell-lists-and-verlet-lists","text":"This example is an extension to Example 2: Cell-lists and Verlet-lists and ()[]. It shows how to use multi-phase cell-lists and Verlet-list using multiple instances of vector_dist . Key points: All the phases have to use the same domain decomposition, which is achieved by passing the decomposition of the first phase to the constructor of vector_dist of all the other phases. The domains have to be iterated individually via getDomainIterator , the particles redistributed via map , the ghost layers synchronized via ghost_get for all the phases vector_dist . Constructing Verlet-lists for two phases ( ph0 , ph1 ) with createVerlet , where for one phase ph0 the neighoring particles of ph1 are assigned in the Verlet-list. Cell-list of ph1 has to be passed to createVerlet Constructing Verlet-lists for multiple phases ( ph0 , ph1 , ph2 ...) with createVerletM , where for one phase ph0 the neighoring particles of ph1 , ph2 ... are assigned in the Verlet-list. Cell-list containing all of ph1 , ph2 ... create with createCellListM has to be passed to createVerletM Iterating over the neighboring particles of a multiphase Verlet-list with getNNIterator with get being substituded by getP (particle phase) and getV (particle id) Extending example of the symmetric interaction for multiphase cell-lists and Verlet-lists via createCellListSymM , createVerletSymM The source code of the example Vector/4_multiphase_celllist_verlet/main.cpp . The full doxygen documentation Vector_4_mp_cl .","title":"Example 14: Multiphase Cell-lists and Verlet-lists"},{"location":"vector-example/#example-16-validation-and-debugging","text":"This example shows how the flexibility of the library can be used to perform complex tasks for validation and debugging. Key points: To get unique global id's of the particles the function accum of vector_dist is used, which returns prefix sum of local domain sizes $j<i$ for the logical processor $i$ out of $N$ total processors Propagate the data from potentially ghost particles q to non-ghost particles in their corresponding domains via ghost_put with the operation merge_ , that merges two openfpm::vector (ghost and non-ghost) The source code of the example Vector/6_complex_usage/main.cpp . The full doxygen documentation Vector_6_complex_usage .","title":"Example 16: Validation and debugging"},{"location":"videos/","text":"OpenFPM Videos OpenFPM-related videos that include recorded talks from workshops, conference presentations and video lessons. OpenFPM a scalable framework for particle and mesh methods Pietro Incardona, August 2018 | C++ Usergroup Dresden 2018 Pietro Incardona presents the architecture and design of OpenFPM, details of the underlying abstractions, and benchmarks of the framework in various applications. The Algorithms of Life - Scientific Computing for Systems Biology Ivo F. Sbalzarini, June 2019 | ISC High Performance 2019 Ivo F. Sbalzarini on the interdisciplinary effort to develop and advance our understanding of life using the high-performance computing approach and the place of the open-source computational frameworks PPM Library, OpenFPM in it. Computational Developmental Biology Ivo F. Sbalzarini, March 2020 | Supercomputing Frontiers Europe 2020 Ivo F. Sbalzarini highlights challenges in computational developmental biology and how they could be tackled with parallel high-performance computing. Video Lessons Particles-1 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 1 Particles-3 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 3 Particles-5 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 5 Particles-7 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 7 Dynamic load balancing-1 Pietro Incardona, February 2017 Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 1 Dynamic load balancing-3 Pietro Incardona, February 2017 Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 3 Visualization Pietro Incardona, August 2018 Video This video lesson shows how to visualize the data output produced by OpenFPM simulations. Particles-2 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 2 Particles-4 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 4 Particles-6 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 6 Particles-8 Pietro Incardona, July 2016 Video This video series introduces the basic use of the distributed data structure vector_dist . Part 8 Dynamic load balancing-2 Pietro Incardona, February 2017 Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 2 Dynamic load balancing-4 Pietro Incardona, February 2017 Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 4","title":"Videos"},{"location":"videos/#openfpm-videos","text":"OpenFPM-related videos that include recorded talks from workshops, conference presentations and video lessons.","title":"OpenFPM Videos"},{"location":"videos/#openfpm-a-scalable-framework-for-particle-and-mesh-methods","text":"","title":"OpenFPM a scalable framework for particle and mesh methods"},{"location":"videos/#pietro-incardona-august-2018-c-usergroup-dresden-2018","text":"Pietro Incardona presents the architecture and design of OpenFPM, details of the underlying abstractions, and benchmarks of the framework in various applications.","title":"Pietro Incardona, August 2018 | C++ Usergroup Dresden 2018"},{"location":"videos/#the-algorithms-of-life-scientific-computing-for-systems-biology","text":"","title":"The Algorithms of Life - Scientific Computing for Systems Biology"},{"location":"videos/#ivo-f-sbalzarini-june-2019-isc-high-performance-2019","text":"Ivo F. Sbalzarini on the interdisciplinary effort to develop and advance our understanding of life using the high-performance computing approach and the place of the open-source computational frameworks PPM Library, OpenFPM in it.","title":"Ivo F. Sbalzarini, June 2019 | ISC High Performance 2019"},{"location":"videos/#computational-developmental-biology","text":"","title":"Computational Developmental Biology"},{"location":"videos/#ivo-f-sbalzarini-march-2020-supercomputing-frontiers-europe-2020","text":"Ivo F. Sbalzarini highlights challenges in computational developmental biology and how they could be tackled with parallel high-performance computing.","title":"Ivo F. Sbalzarini, March 2020 | Supercomputing Frontiers Europe 2020"},{"location":"videos/#video-lessons","text":"","title":"Video Lessons"},{"location":"videos/#particles-1","text":"","title":"Particles-1"},{"location":"videos/#pietro-incardona-july-2016","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 1","title":"Pietro Incardona, July 2016"},{"location":"videos/#particles-3","text":"","title":"Particles-3"},{"location":"videos/#pietro-incardona-july-2016_1","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 3","title":"Pietro Incardona, July 2016"},{"location":"videos/#particles-5","text":"","title":"Particles-5"},{"location":"videos/#pietro-incardona-july-2016_2","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 5","title":"Pietro Incardona, July 2016"},{"location":"videos/#particles-7","text":"","title":"Particles-7"},{"location":"videos/#pietro-incardona-july-2016_3","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 7","title":"Pietro Incardona, July 2016"},{"location":"videos/#dynamic-load-balancing-1","text":"","title":"Dynamic load balancing-1"},{"location":"videos/#pietro-incardona-february-2017","text":"Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 1","title":"Pietro Incardona, February 2017"},{"location":"videos/#dynamic-load-balancing-3","text":"","title":"Dynamic load balancing-3"},{"location":"videos/#pietro-incardona-february-2017_1","text":"Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 3","title":"Pietro Incardona, February 2017"},{"location":"videos/#visualization","text":"","title":"Visualization"},{"location":"videos/#pietro-incardona-august-2018","text":"Video This video lesson shows how to visualize the data output produced by OpenFPM simulations.","title":"Pietro Incardona, August 2018"},{"location":"videos/#particles-2","text":"","title":"Particles-2"},{"location":"videos/#pietro-incardona-july-2016_4","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 2","title":"Pietro Incardona, July 2016"},{"location":"videos/#particles-4","text":"","title":"Particles-4"},{"location":"videos/#pietro-incardona-july-2016_5","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 4","title":"Pietro Incardona, July 2016"},{"location":"videos/#particles-6","text":"","title":"Particles-6"},{"location":"videos/#pietro-incardona-july-2016_6","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 6","title":"Pietro Incardona, July 2016"},{"location":"videos/#particles-8","text":"","title":"Particles-8"},{"location":"videos/#pietro-incardona-july-2016_7","text":"Video This video series introduces the basic use of the distributed data structure vector_dist . Part 8","title":"Pietro Incardona, July 2016"},{"location":"videos/#dynamic-load-balancing-2","text":"","title":"Dynamic load balancing-2"},{"location":"videos/#pietro-incardona-february-2017_2","text":"Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 2","title":"Pietro Incardona, February 2017"},{"location":"videos/#dynamic-load-balancing-4","text":"","title":"Dynamic load balancing-4"},{"location":"videos/#pietro-incardona-february-2017_3","text":"Video This video series extends the series Particles and introduces the problem of load balancing and dynamic load balancing. It illustrates the practical use-case of the distributed data structure vector_dist . Part 4","title":"Pietro Incardona, February 2017"}]}